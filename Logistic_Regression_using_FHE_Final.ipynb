{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siya-9/Homomorphic-Encryption/blob/main/Logistic_Regression_using_FHE_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv8Sep_l-6Ck",
        "outputId": "c0d00022-36c5-44c4-adb4-3fbbdcb5da56"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FbCzQ-rlmyR",
        "outputId": "aea2b033-c6d4-4ae5-95e5-980dc988f503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tenseal\n",
            "  Downloading tenseal-0.3.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenseal\n",
            "Successfully installed tenseal-0.3.14\n"
          ]
        }
      ],
      "source": [
        "!pip install tenseal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3S3jd_IPl3aB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tenseal as ts\n",
        "import pandas as pd\n",
        "import random\n",
        "from time import time\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exTzulzP5j-a",
        "outputId": "227bc943-b5bb-426c-831d-3ae7df23ceae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ee1ae056c66a>:8: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  data = data.drop(\"Exited\", 'columns')\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/datasets/bank.csv\")\n",
        "# drop rows with missing values\n",
        "data = data.dropna()\n",
        "data = data[:-5000]\n",
        "# drop some features\n",
        "data = data.drop(columns=[\"CustomerId\", \"Geography\"])\n",
        "y = data['Exited'].values\n",
        "data = data.drop(\"Exited\", 'columns')\n",
        "X = data.values\n",
        "\n",
        "print(type(X), type(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w0BTFslJoJPQ"
      },
      "outputs": [],
      "source": [
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=124)\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mcZdGjxDof06"
      },
      "outputs": [],
      "source": [
        "class LRModel(torch.nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LRModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "model = LRModel(n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fkEePKwSopf0"
      },
      "outputs": [],
      "source": [
        "num_epochs = 300\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.BCELoss()\n",
        "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cm_pPMncouH7"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Backward pass and update\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    # zero grad before new step\n",
        "    optim.zero_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W9Cz_KQpY0W",
        "outputId": "76e0500f-aca8-489f-f18c-73f50010b979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.7647\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    y_predicted = model(X_test)\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "    print(f'accuracy: {acc.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hONeDcdEpdjo"
      },
      "outputs": [],
      "source": [
        "class EncryptedLR:\n",
        "\n",
        "    def __init__(self, torch_lr):\n",
        "        # TenSEAL processes lists and not torch tensors, so we take out the parameters from the PyTorch model\n",
        "        self.weight = torch_lr.linear.weight.data.tolist()[0]\n",
        "        self.bias = torch_lr.linear.bias.data.tolist()\n",
        "\n",
        "    def forward(self, enc_x):\n",
        "        enc_out = enc_x.dot(self.weight) + self.bias\n",
        "        return enc_out\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "\n",
        "    def encrypt(self, context):\n",
        "        self.weight = ts.ckks_vector(context, self.weight)\n",
        "        self.bias = ts.ckks_vector(context, self.bias)\n",
        "\n",
        "    def decrypt(self, context):\n",
        "        self.weight = self.weight.decrypt()\n",
        "        self.bias = self.bias.decrypt()\n",
        "\n",
        "\n",
        "eelr = EncryptedLR(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-0wLG612rHXL"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "poly_mod_degree = 4096\n",
        "coeff_mod_bit_sizes = [40, 20, 40]\n",
        "# create TenSEALContext\n",
        "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
        "# scale of ciphertext to use\n",
        "ctx_eval.global_scale = 2 ** 20\n",
        "# this key is needed for doing dot-product operations\n",
        "ctx_eval.generate_galois_keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-l0qPoHrv1K",
        "outputId": "0f04f758-39bd-4971-eef6-8750b64b2237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encryption of the test-set took 3 seconds\n"
          ]
        }
      ],
      "source": [
        "t_start = time()\n",
        "enc_x_test = [ts.ckks_vector(ctx_eval, x.tolist()) for x in X_test]\n",
        "t_end = time()\n",
        "print(f\"Encryption of the test-set took {int(t_end - t_start)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGSGPcL1rxgb",
        "outputId": "c7a64bc6-fe7f-405d-fcd3-8328ecee80fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated test_set of 5000 entries in 3 seconds\n",
            "Accuracy: 1125/1500 = 0.75\n"
          ]
        }
      ],
      "source": [
        "def encrypted_evaluation(model, enc_x_test, y_test):\n",
        "    t_start = time()\n",
        "\n",
        "    correct = 0\n",
        "    for enc_x, y in zip(enc_x_test, y_test):\n",
        "        # encrypted evaluation\n",
        "        enc_out = model(enc_x)\n",
        "        # plain comparison\n",
        "        out = enc_out.decrypt()\n",
        "        out = torch.tensor(out)\n",
        "        out = torch.sigmoid(out)\n",
        "        if torch.abs(out - y) < 0.5:\n",
        "            correct += 1\n",
        "\n",
        "    t_end = time()\n",
        "    print(f\"Evaluated test_set of {len(X)} entries in {int(t_end - t_start)} seconds\")\n",
        "    print(f\"Accuracy: {correct}/{len(X_test)} = {correct / len(X_test)}\")\n",
        "    return correct / len(X_test)\n",
        "\n",
        "\n",
        "encrypted_accuracy = encrypted_evaluation(eelr, enc_x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rAxT4WASsHxV"
      },
      "outputs": [],
      "source": [
        "class EncryptedLR:\n",
        "\n",
        "    def __init__(self, torch_lr):\n",
        "        self.weight = torch_lr.linear.weight.data.tolist()[0]\n",
        "        self.bias = torch_lr.linear.bias.data.tolist()\n",
        "        # we accumulate gradients and counts the number of iterations\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "        self._count = 0\n",
        "\n",
        "    def forward(self, enc_x):\n",
        "        enc_out = enc_x.dot(self.weight) + self.bias\n",
        "        enc_out = EncryptedLR.sigmoid(enc_out)\n",
        "        return enc_out\n",
        "\n",
        "    def backward(self, enc_x, enc_out, enc_y):\n",
        "        out_minus_y = (enc_out - enc_y)\n",
        "        self._delta_w += enc_x * out_minus_y\n",
        "        self._delta_b += out_minus_y\n",
        "        self._count += 1\n",
        "\n",
        "    def update_parameters(self):\n",
        "        if self._count == 0:\n",
        "            raise RuntimeError(\"You should at least run one forward iteration\")\n",
        "        # update weights\n",
        "        # We use a small regularization term to keep the output\n",
        "        # of the linear layer in the range of the sigmoid approximation\n",
        "        self.weight -= self._delta_w * (1 / self._count) + self.weight * 0.01\n",
        "        self.bias -= self._delta_b * (1 / self._count)\n",
        "        # reset gradient accumulators and iterations count\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "        self._count = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(enc_x):\n",
        "        # We use the polynomial approximation of degree 3 sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3 which fits the function pretty well in the range [-5,5]\n",
        "        return enc_x.polyval([0.5, 0.197, 0, -0.004])\n",
        "\n",
        "    def plain_accuracy(self, x_test, y_test):\n",
        "        # evaluate accuracy of the model on the plain (x_test, y_test) dataset\n",
        "        w = torch.tensor(self.weight)\n",
        "        b = torch.tensor(self.bias)\n",
        "        out = torch.sigmoid(x_test.matmul(w) + b).reshape(-1, 1)\n",
        "        correct = torch.abs(y_test - out) < 0.5\n",
        "        return correct.float().mean()\n",
        "\n",
        "    def encrypt(self, context):\n",
        "        self.weight = ts.ckks_vector(context, self.weight)\n",
        "        self.bias = ts.ckks_vector(context, self.bias)\n",
        "\n",
        "    def decrypt(self):\n",
        "        self.weight = self.weight.decrypt()\n",
        "        self.bias = self.bias.decrypt()\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ftVONlJIsWY8"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "poly_mod_degree = 8192\n",
        "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
        "# create TenSEALContext\n",
        "ctx_training = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
        "ctx_training.global_scale = 2 ** 21\n",
        "ctx_training.generate_galois_keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmihQeqosYiV",
        "outputId": "79f504d3-439d-4713-ab7b-240c94c8cfb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encryption of the training_set took 60 seconds\n"
          ]
        }
      ],
      "source": [
        "t_start = time()\n",
        "enc_x_train = [ts.ckks_vector(ctx_training, x.tolist()) for x in X_train]\n",
        "enc_y_train = [ts.ckks_vector(ctx_training, y.tolist()) for y in y_train]\n",
        "t_end = time()\n",
        "print(f\"Encryption of the training_set took {int(t_end - t_start)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "D1k2lElvsbax"
      },
      "outputs": [],
      "source": [
        "# normal_dist = lambda x, mean, var: np.exp(- np.square(x - mean) / (2 * var)) / np.sqrt(2 * np.pi * var)\n",
        "\n",
        "# def plot_normal_dist(mean, var, rmin=-10, rmax=10):\n",
        "#     x = np.arange(rmin, rmax, 0.01)\n",
        "#     y = normal_dist(x, mean, var)\n",
        "#     fig = plt.plot(x, y)\n",
        "\n",
        "# # plain distribution\n",
        "# lr = LRModel(n_features)\n",
        "# data = lr.linear(X_test)\n",
        "# mean, var = map(float, [data.mean(), data.std() ** 2])\n",
        "# plot_normal_dist(mean, var)\n",
        "# print(\"Distribution on plain data:\")\n",
        "# plt.show()\n",
        "\n",
        "# # encrypted distribution\n",
        "# def encrypted_out_distribution(eelr, enc_x_test):\n",
        "#     w = eelr.weight\n",
        "#     b = eelr.bias\n",
        "#     data = []\n",
        "#     for enc_x in enc_x_test:\n",
        "#         enc_out = enc_x.dot(w) + b\n",
        "#         data.append(enc_out.decrypt())\n",
        "#     data = torch.tensor(data)\n",
        "#     mean, var = map(float, [data.mean(), data.std() ** 2])\n",
        "#     plot_normal_dist(mean, var)\n",
        "#     print(\"Distribution on encrypted data:\")\n",
        "#     plt.show()\n",
        "\n",
        "# eelr = EncryptedLR(lr)\n",
        "# eelr.encrypt(ctx_training)\n",
        "# encrypted_out_distribution(eelr, enc_x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGpgYyyrsiEQ",
        "outputId": "510e52ff-7acd-4d96-eaaa-5440d671a9c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy at epoch #0 is 0.5366666913032532\n",
            "Accuracy at epoch #1 is 0.7586666941642761\n",
            "Accuracy at epoch #2 is 0.7900000214576721\n",
            "Accuracy at epoch #3 is 0.7826666831970215\n",
            "Accuracy at epoch #4 is 0.7746666669845581\n",
            "Accuracy at epoch #5 is 0.7760000228881836\n",
            "\n",
            "Average time per epoch: 312 seconds\n",
            "Final accuracy is 0.7760000228881836\n"
          ]
        }
      ],
      "source": [
        "eelr = EncryptedLR(LRModel(n_features))\n",
        "accuracy = eelr.plain_accuracy(X_test, y_test)\n",
        "print(f\"Accuracy at epoch #0 is {accuracy}\")\n",
        "\n",
        "times = []\n",
        "for epoch in range(5):\n",
        "    eelr.encrypt(ctx_training)\n",
        "    t_start = time()\n",
        "    for enc_x, enc_y in zip(enc_x_train, enc_y_train):\n",
        "        enc_out = eelr.forward(enc_x)\n",
        "        eelr.backward(enc_x, enc_out, enc_y)\n",
        "    eelr.update_parameters()\n",
        "    t_end = time()\n",
        "    times.append(t_end - t_start)\n",
        "\n",
        "    eelr.decrypt()\n",
        "    accuracy = eelr.plain_accuracy(X_test, y_test)\n",
        "    print(f\"Accuracy at epoch #{epoch + 1} is {accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nAverage time per epoch: {int(sum(times) / len(times))} seconds\")\n",
        "print(f\"Final accuracy is {accuracy}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEXe8T_nUWet",
        "outputId": "c18b7412-8170-42ec-887a-b6fa5ce71f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference between plain and encrypted accuracies: -0.011333346366882324\n",
            "We got a better accuracy when training on encrypted data\n"
          ]
        }
      ],
      "source": [
        "diff_accuracy = acc - accuracy\n",
        "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")\n",
        "if diff_accuracy < 0:\n",
        "    print(\"We got a better accuracy when training on encrypted data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jhv7jxtQUejS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPDrCAgrxKKx"
      },
      "outputs": [],
      "source": [
        "# plain_accuracy = 0.7687\n",
        "# accuracy = 0.7860"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1H6gmYVgXf56wUOXgPghlQANvg8ONrNRX",
      "authorship_tag": "ABX9TyM7nlchYQNsvguodRYkflWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}