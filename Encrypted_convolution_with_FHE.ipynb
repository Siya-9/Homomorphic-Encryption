{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvWzQjJfyZMycSV52lSmDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siya-9/Homomorphic-Encryption/blob/main/Encrypted_convolution_with_FHE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(73)\n",
        "\n",
        "train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw7R_q4MacY4",
        "outputId": "c26071a6-4c9d-4943-8fe3-63953473eec7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16561389.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 512986.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4510582.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4809525.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(torch.nn.Module):\n",
        "    def __init__(self, hidden=64, output=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=7, padding=0, stride=3)\n",
        "        self.fc1 = torch.nn.Linear(256, hidden)\n",
        "        self.fc2 = torch.nn.Linear(hidden, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        # the model uses the square activation function\n",
        "        x = x * x\n",
        "        # flattening while keeping the batch axis\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.fc1(x)\n",
        "        x = x * x\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "ir_Ay0Zdac_8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, n_epochs=10):\n",
        "    # model in training mode\n",
        "    model.train()\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "\n",
        "        train_loss = 0.0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # calculate average losses\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
        "\n",
        "    # model in evaluation mode\n",
        "    model.eval()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "dJ2nzoXCcHFr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model = train(model, train_loader, criterion, optimizer, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7Hu-ZN6cJJy",
        "outputId": "b2ec1a3c-a754-4d63-a121-566bf40a649a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.397561\n",
            "Epoch: 2 \tTraining Loss: 0.130699\n",
            "Epoch: 3 \tTraining Loss: 0.088399\n",
            "Epoch: 4 \tTraining Loss: 0.071318\n",
            "Epoch: 5 \tTraining Loss: 0.058989\n",
            "Epoch: 6 \tTraining Loss: 0.050542\n",
            "Epoch: 7 \tTraining Loss: 0.044438\n",
            "Epoch: 8 \tTraining Loss: 0.038255\n",
            "Epoch: 9 \tTraining Loss: 0.034721\n",
            "Epoch: 10 \tTraining Loss: 0.030650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above operation takes 2 mins."
      ],
      "metadata": {
        "id": "Nm68FB8vKXww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, criterion):\n",
        "    # initialize lists to monitor test loss and accuracy\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    # model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "    # calculate and print avg test loss\n",
        "    test_loss = test_loss/len(test_loader)\n",
        "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
        "\n",
        "    for label in range(10):\n",
        "        print(\n",
        "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
        "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% '\n",
        "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
        "    )\n",
        "\n",
        "test(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcd9qbNGdXK4",
        "outputId": "f90bbb44-d254-4157-cdb1-730d861d4665"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.077934\n",
            "\n",
            "Test Accuracy of 0: 98% (970/980)\n",
            "Test Accuracy of 1: 99% (1128/1135)\n",
            "Test Accuracy of 2: 98% (1013/1032)\n",
            "Test Accuracy of 3: 99% (1004/1010)\n",
            "Test Accuracy of 4: 98% (970/982)\n",
            "Test Accuracy of 5: 98% (876/892)\n",
            "Test Accuracy of 6: 98% (945/958)\n",
            "Test Accuracy of 7: 98% (1008/1028)\n",
            "Test Accuracy of 8: 97% (950/974)\n",
            "Test Accuracy of 9: 96% (973/1009)\n",
            "\n",
            "Test Accuracy (Overall): 98% (9837/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenseal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J5VEl8fdrlm",
        "outputId": "648e8ffc-95eb-4ba0-fd54-d45421a89377"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tenseal\n",
            "  Downloading tenseal-0.3.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenseal\n",
            "Successfully installed tenseal-0.3.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It's a PyTorch-like model using operations implemented in TenSEAL.\n",
        "    - .mm() method is doing the vector-matrix multiplication explained above.\n",
        "    - you can use + operator to add a plain vector as a bias.\n",
        "    - .conv2d_im2col() method is doing a single convolution operation.\n",
        "    - .square_() just square the encrypted vector inplace.\n",
        "\"\"\"\n",
        "\n",
        "import tenseal as ts\n",
        "\n",
        "\n",
        "class EncConvNet:\n",
        "    def __init__(self, torch_nn):\n",
        "        self.conv1_weight = torch_nn.conv1.weight.data.view(\n",
        "            torch_nn.conv1.out_channels, torch_nn.conv1.kernel_size[0],\n",
        "            torch_nn.conv1.kernel_size[1]\n",
        "        ).tolist()\n",
        "        self.conv1_bias = torch_nn.conv1.bias.data.tolist()\n",
        "\n",
        "        self.fc1_weight = torch_nn.fc1.weight.T.data.tolist()\n",
        "        self.fc1_bias = torch_nn.fc1.bias.data.tolist()\n",
        "\n",
        "        self.fc2_weight = torch_nn.fc2.weight.T.data.tolist()\n",
        "        self.fc2_bias = torch_nn.fc2.bias.data.tolist()\n",
        "\n",
        "\n",
        "    def forward(self, enc_x, windows_nb):\n",
        "        # conv layer\n",
        "        enc_channels = []\n",
        "        for kernel, bias in zip(self.conv1_weight, self.conv1_bias):\n",
        "            y = enc_x.conv2d_im2col(kernel, windows_nb) + bias\n",
        "            enc_channels.append(y)\n",
        "        # pack all channels into a single flattened vector\n",
        "        enc_x = ts.CKKSVector.pack_vectors(enc_channels)\n",
        "        # square activation\n",
        "        enc_x.square_()\n",
        "        # fc1 layer\n",
        "        enc_x = enc_x.mm(self.fc1_weight) + self.fc1_bias\n",
        "        # square activation\n",
        "        enc_x.square_()\n",
        "        # fc2 layer\n",
        "        enc_x = enc_x.mm(self.fc2_weight) + self.fc2_bias\n",
        "        return enc_x\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n"
      ],
      "metadata": {
        "id": "5DVCL-IbdlgB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enc_test(context, model, test_loader, criterion, kernel_shape, stride):\n",
        "    # initialize lists to monitor test loss and accuracy\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    epoch = 0\n",
        "    for data, target in test_loader:\n",
        "        epoch += 1\n",
        "        # Encoding and encryption\n",
        "        x_enc, windows_nb = ts.im2col_encoding(\n",
        "            context, data.view(28, 28).tolist(), kernel_shape[0],\n",
        "            kernel_shape[1], stride\n",
        "        )\n",
        "        # Encrypted evaluation\n",
        "        enc_output = enc_model(x_enc, windows_nb)\n",
        "        # Decryption of result\n",
        "        output = enc_output.decrypt()\n",
        "        output = torch.tensor(output).view(1, -1)\n",
        "\n",
        "        # compute loss\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        print(f'At epoch {epoch}, test loss: {test_loss}')\n",
        "\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        label = target.data[0]\n",
        "        class_correct[label] += correct.item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "\n",
        "    # calculate and print avg test loss\n",
        "    test_loss = test_loss / sum(class_total)\n",
        "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
        "\n",
        "    for label in range(10):\n",
        "        print(\n",
        "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
        "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% '\n",
        "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "AQoC1x3HdoLy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, filename):\n",
        "    model_params = {\n",
        "        'conv1_weight': model.conv1_weight,\n",
        "        'conv1_bias': model.conv1_bias,\n",
        "        'fc1_weight': model.fc1_weight,\n",
        "        'fc1_bias': model.fc1_bias,\n",
        "        'fc2_weight': model.fc2_weight,\n",
        "        'fc2_bias': model.fc2_bias\n",
        "    }\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model_params, f)\n",
        "\n",
        "def load_model(filename, torch_nn):\n",
        "    with open(filename, 'rb') as f:\n",
        "        model_params = pickle.load(f)\n",
        "\n",
        "    model = EncConvNet(torch_nn)\n",
        "    model.conv1_weight = model_params['conv1_weight']\n",
        "    model.conv1_bias = model_params['conv1_bias']\n",
        "    model.fc1_weight = model_params['fc1_weight']\n",
        "    model.fc1_bias = model_params['fc1_bias']\n",
        "    model.fc2_weight = model_params['fc2_weight']\n",
        "    model.fc2_bias = model_params['fc2_bias']\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9OebS1uKXuqB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test_samples = int(len(test_data)  * 0.1)\n",
        "test_indices = np.random.choice(len(test_data), size=num_test_samples, replace=False)\n",
        "test_data = torch.utils.data.Subset(test_data, test_indices)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "# Load one element at a time\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
        "# required for encoding\n",
        "kernel_shape = model.conv1.kernel_size\n",
        "stride = model.conv1.stride[0]"
      ],
      "metadata": {
        "id": "cR_SYedwis7C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Encryption Parameters\n",
        "\n",
        "# controls precision of the fractional part\n",
        "bits_scale = 26\n",
        "\n",
        "# Create TenSEAL context\n",
        "context = ts.context(\n",
        "    ts.SCHEME_TYPE.CKKS,\n",
        "    poly_modulus_degree=8192,\n",
        "    coeff_mod_bit_sizes=[31, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, 31]\n",
        ")\n",
        "\n",
        "# set the scale\n",
        "context.global_scale = pow(2, bits_scale)\n",
        "\n",
        "# galois keys are required to do ciphertext rotations\n",
        "context.generate_galois_keys()"
      ],
      "metadata": {
        "id": "W9c8xTMld04G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_model = EncConvNet(model)"
      ],
      "metadata": {
        "id": "u_PFNKN2d550"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "-X2Jkoa5ZcuU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(enc_model, 'enc_convnet_model.pkl')\n",
        "\n",
        "enc_model = load_model('enc_convnet_model.pkl', model)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "s8C0whqHX-LF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_test(context, enc_model, test_loader, criterion, kernel_shape, stride)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldFUhGVohr1y",
        "outputId": "6b86d440-a0a0-46b8-8ddd-21c209b26ec3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At epoch 1, test loss: 1.1920928244535389e-07\n",
            "At epoch 2, test loss: 1.1920928244535389e-07\n",
            "At epoch 3, test loss: 1.1920928244535389e-07\n",
            "At epoch 4, test loss: 1.1920928244535389e-07\n",
            "At epoch 5, test loss: 4.136477217997481e-05\n",
            "At epoch 6, test loss: 0.00011467580042534564\n",
            "At epoch 7, test loss: 0.00011467580042534564\n",
            "At epoch 8, test loss: 0.00012182833233254087\n",
            "At epoch 9, test loss: 0.00012194754161498622\n",
            "At epoch 10, test loss: 0.00012194754161498622\n",
            "At epoch 11, test loss: 0.00012337805206641406\n",
            "At epoch 12, test loss: 0.008564031845580189\n",
            "At epoch 13, test loss: 0.008575475871893445\n",
            "At epoch 14, test loss: 0.008575475871893445\n",
            "At epoch 15, test loss: 0.008575475871893445\n",
            "At epoch 16, test loss: 0.0085846549453521\n",
            "At epoch 17, test loss: 0.0085866815012281\n",
            "At epoch 18, test loss: 0.14620915712531257\n",
            "At epoch 19, test loss: 0.14620951475312438\n",
            "At epoch 20, test loss: 0.14620951475312438\n",
            "At epoch 21, test loss: 0.14620951475312438\n",
            "At epoch 22, test loss: 0.14620951475312438\n",
            "At epoch 23, test loss: 0.14621154130900038\n",
            "At epoch 24, test loss: 0.14678989670237286\n",
            "At epoch 25, test loss: 0.1468288773797184\n",
            "At epoch 26, test loss: 0.14696834252781343\n",
            "At epoch 27, test loss: 0.1469692962016751\n",
            "At epoch 28, test loss: 0.147435534239591\n",
            "At epoch 29, test loss: 0.14743565344887344\n",
            "At epoch 30, test loss: 0.14743565344887344\n",
            "At epoch 31, test loss: 0.14743577265815588\n",
            "At epoch 32, test loss: 0.14743589186743833\n",
            "At epoch 33, test loss: 0.14743589186743833\n",
            "At epoch 34, test loss: 0.14743589186743833\n",
            "At epoch 35, test loss: 0.14743624949525014\n",
            "At epoch 36, test loss: 0.14743624949525014\n",
            "At epoch 37, test loss: 0.14743624949525014\n",
            "At epoch 38, test loss: 0.14743624949525014\n",
            "At epoch 39, test loss: 0.14743624949525014\n",
            "At epoch 40, test loss: 0.14756617918074966\n",
            "At epoch 41, test loss: 0.14756617918074966\n",
            "At epoch 42, test loss: 0.14756617918074966\n",
            "At epoch 43, test loss: 0.14756617918074966\n",
            "At epoch 44, test loss: 0.14756617918074966\n",
            "At epoch 45, test loss: 0.14762518603648545\n",
            "At epoch 46, test loss: 0.14763031202274846\n",
            "At epoch 47, test loss: 0.14763305383254277\n",
            "At epoch 48, test loss: 0.14764211369762137\n",
            "At epoch 49, test loss: 0.14764235211617205\n",
            "At epoch 50, test loss: 0.14764235211617205\n",
            "At epoch 51, test loss: 0.14764235211617205\n",
            "At epoch 52, test loss: 0.14764306737168198\n",
            "At epoch 53, test loss: 0.14769158437382401\n",
            "At epoch 54, test loss: 0.14769158437382401\n",
            "At epoch 55, test loss: 0.1476927764660374\n",
            "At epoch 56, test loss: 0.1477101808713357\n",
            "At epoch 57, test loss: 0.1477101808713357\n",
            "At epoch 58, test loss: 0.14771041928988637\n",
            "At epoch 59, test loss: 0.14771459160638045\n",
            "At epoch 60, test loss: 0.14771459160638045\n",
            "At epoch 61, test loss: 0.14771459160638045\n",
            "At epoch 62, test loss: 0.14771459160638045\n",
            "At epoch 63, test loss: 0.14771459160638045\n",
            "At epoch 64, test loss: 0.14771459160638045\n",
            "At epoch 65, test loss: 0.14772758533435848\n",
            "At epoch 66, test loss: 0.14772758533435848\n",
            "At epoch 67, test loss: 0.14786597774436672\n",
            "At epoch 68, test loss: 0.14786633537217853\n",
            "At epoch 69, test loss: 0.14787563365310774\n",
            "At epoch 70, test loss: 0.1559747286176716\n",
            "At epoch 71, test loss: 0.1559747286176716\n",
            "At epoch 72, test loss: 0.1559747286176716\n",
            "At epoch 73, test loss: 0.1559747286176716\n",
            "At epoch 74, test loss: 0.15632573836258956\n",
            "At epoch 75, test loss: 0.15632573836258956\n",
            "At epoch 76, test loss: 0.15632573836258956\n",
            "At epoch 77, test loss: 0.2606884496729762\n",
            "At epoch 78, test loss: 0.2606884496729762\n",
            "At epoch 79, test loss: 0.4135355608981044\n",
            "At epoch 80, test loss: 0.4135355608981044\n",
            "At epoch 81, test loss: 1.0139112728159816\n",
            "At epoch 82, test loss: 1.0139112728159816\n",
            "At epoch 83, test loss: 1.013912464908195\n",
            "At epoch 84, test loss: 1.013912464908195\n",
            "At epoch 85, test loss: 1.0485301422391657\n",
            "At epoch 86, test loss: 1.3247752296243434\n",
            "At epoch 87, test loss: 1.3247859584031048\n",
            "At epoch 88, test loss: 1.3247859584031048\n",
            "At epoch 89, test loss: 1.3250596254752836\n",
            "At epoch 90, test loss: 1.3250596254752836\n",
            "At epoch 91, test loss: 1.3253286446657384\n",
            "At epoch 92, test loss: 1.3253286446657384\n",
            "At epoch 93, test loss: 1.3253286446657384\n",
            "At epoch 94, test loss: 1.3253286446657384\n",
            "At epoch 95, test loss: 1.3253287638750209\n",
            "At epoch 96, test loss: 1.3253287638750209\n",
            "At epoch 97, test loss: 1.3253287638750209\n",
            "At epoch 98, test loss: 1.325345691450437\n",
            "At epoch 99, test loss: 1.3256147106408918\n",
            "At epoch 100, test loss: 1.3256608435700201\n",
            "At epoch 101, test loss: 1.3256608435700201\n",
            "At epoch 102, test loss: 1.3256608435700201\n",
            "At epoch 103, test loss: 1.3256608435700201\n",
            "At epoch 104, test loss: 1.325776589089017\n",
            "At epoch 105, test loss: 1.3337549455323625\n",
            "At epoch 106, test loss: 1.3337558992062242\n",
            "At epoch 107, test loss: 1.3337631709465114\n",
            "At epoch 108, test loss: 1.3337631709465114\n",
            "At epoch 109, test loss: 1.3337631709465114\n",
            "At epoch 110, test loss: 1.3337631709465114\n",
            "At epoch 111, test loss: 1.33383433636034\n",
            "At epoch 112, test loss: 1.3338345747788907\n",
            "At epoch 113, test loss: 1.3339235009580506\n",
            "At epoch 114, test loss: 1.3339235009580506\n",
            "At epoch 115, test loss: 1.3339235009580506\n",
            "At epoch 116, test loss: 1.3339235009580506\n",
            "At epoch 117, test loss: 1.3339237393766012\n",
            "At epoch 118, test loss: 1.3339237393766012\n",
            "At epoch 119, test loss: 1.3341385314515861\n",
            "At epoch 120, test loss: 1.3341385314515861\n",
            "At epoch 121, test loss: 1.3341385314515861\n",
            "At epoch 122, test loss: 1.3341440150638988\n",
            "At epoch 123, test loss: 1.3341440150638988\n",
            "At epoch 124, test loss: 1.3341440150638988\n",
            "At epoch 125, test loss: 1.3341440150638988\n",
            "At epoch 126, test loss: 1.3341448495285846\n",
            "At epoch 127, test loss: 1.3341601082012318\n",
            "At epoch 128, test loss: 1.4847832360193394\n",
            "At epoch 129, test loss: 1.4847832360193394\n",
            "At epoch 130, test loss: 1.4847832360193394\n",
            "At epoch 131, test loss: 1.4847832360193394\n",
            "At epoch 132, test loss: 1.4847942032139514\n",
            "At epoch 133, test loss: 1.6614421643536303\n",
            "At epoch 134, test loss: 1.6614421643536303\n",
            "At epoch 135, test loss: 1.6614421643536303\n",
            "At epoch 136, test loss: 1.661442402772181\n",
            "At epoch 137, test loss: 1.661442402772181\n",
            "At epoch 138, test loss: 1.661442402772181\n",
            "At epoch 139, test loss: 1.661442402772181\n",
            "At epoch 140, test loss: 1.661442402772181\n",
            "At epoch 141, test loss: 1.661442402772181\n",
            "At epoch 142, test loss: 1.661442402772181\n",
            "At epoch 143, test loss: 1.661442402772181\n",
            "At epoch 144, test loss: 1.661442402772181\n",
            "At epoch 145, test loss: 1.661442402772181\n",
            "At epoch 146, test loss: 1.661442402772181\n",
            "At epoch 147, test loss: 1.6622868823106813\n",
            "At epoch 148, test loss: 1.6622868823106813\n",
            "At epoch 149, test loss: 1.663419206066564\n",
            "At epoch 150, test loss: 1.6634242128439922\n",
            "At epoch 151, test loss: 1.6736977634810586\n",
            "At epoch 152, test loss: 1.6736990747823342\n",
            "At epoch 153, test loss: 1.6736990747823342\n",
            "At epoch 154, test loss: 1.6736990747823342\n",
            "At epoch 155, test loss: 1.6736990747823342\n",
            "At epoch 156, test loss: 1.674720891410388\n",
            "At epoch 157, test loss: 1.6747212490381997\n",
            "At epoch 158, test loss: 1.674777514240347\n",
            "At epoch 159, test loss: 1.674777514240347\n",
            "At epoch 160, test loss: 1.6747782294958569\n",
            "At epoch 161, test loss: 1.6748203094910394\n",
            "At epoch 162, test loss: 1.6748203094910394\n",
            "At epoch 163, test loss: 1.676951525486217\n",
            "At epoch 164, test loss: 1.6769543865050736\n",
            "At epoch 165, test loss: 1.6774430252083192\n",
            "At epoch 166, test loss: 1.6774430252083192\n",
            "At epoch 167, test loss: 1.6774430252083192\n",
            "At epoch 168, test loss: 1.6775561484285362\n",
            "At epoch 169, test loss: 1.6775561484285362\n",
            "At epoch 170, test loss: 1.6775561484285362\n",
            "At epoch 171, test loss: 1.6780344209294498\n",
            "At epoch 172, test loss: 1.6780344209294498\n",
            "At epoch 173, test loss: 1.6780344209294498\n",
            "At epoch 174, test loss: 1.6780345401387322\n",
            "At epoch 175, test loss: 2.3128736576657403\n",
            "At epoch 176, test loss: 2.3128736576657403\n",
            "At epoch 177, test loss: 2.3128736576657403\n",
            "At epoch 178, test loss: 2.3128736576657403\n",
            "At epoch 179, test loss: 2.3135563745638947\n",
            "At epoch 180, test loss: 3.076156081213675\n",
            "At epoch 181, test loss: 3.076241908220517\n",
            "At epoch 182, test loss: 3.076241908220517\n",
            "At epoch 183, test loss: 3.076241908220517\n",
            "At epoch 184, test loss: 3.076241908220517\n",
            "At epoch 185, test loss: 3.0927572414259785\n",
            "At epoch 186, test loss: 3.0927572414259785\n",
            "At epoch 187, test loss: 3.0927575990537903\n",
            "At epoch 188, test loss: 3.0927575990537903\n",
            "At epoch 189, test loss: 3.092758910355066\n",
            "At epoch 190, test loss: 3.1029128111870037\n",
            "At epoch 191, test loss: 3.102913884070041\n",
            "At epoch 192, test loss: 3.102913884070041\n",
            "At epoch 193, test loss: 3.102926162551377\n",
            "At epoch 194, test loss: 3.1029285467344394\n",
            "At epoch 195, test loss: 3.102943447784675\n",
            "At epoch 196, test loss: 3.1029435669939573\n",
            "At epoch 197, test loss: 3.1029435669939573\n",
            "At epoch 198, test loss: 3.103025818020427\n",
            "At epoch 199, test loss: 3.103025818020427\n",
            "At epoch 200, test loss: 3.1030321360927644\n",
            "At epoch 201, test loss: 3.1030321360927644\n",
            "At epoch 202, test loss: 3.1030321360927644\n",
            "At epoch 203, test loss: 3.1030321360927644\n",
            "At epoch 204, test loss: 3.1030321360927644\n",
            "At epoch 205, test loss: 3.1030431032873764\n",
            "At epoch 206, test loss: 3.1030431032873764\n",
            "At epoch 207, test loss: 3.103595724026121\n",
            "At epoch 208, test loss: 3.103629221276236\n",
            "At epoch 209, test loss: 3.1036296981132807\n",
            "At epoch 210, test loss: 3.1036296981132807\n",
            "At epoch 211, test loss: 3.1036296981132807\n",
            "At epoch 212, test loss: 3.1036296981132807\n",
            "At epoch 213, test loss: 3.1036296981132807\n",
            "At epoch 214, test loss: 3.1036296981132807\n",
            "At epoch 215, test loss: 3.103707300348532\n",
            "At epoch 216, test loss: 3.103707300348532\n",
            "At epoch 217, test loss: 3.103707300348532\n",
            "At epoch 218, test loss: 3.103707300348532\n",
            "At epoch 219, test loss: 3.1053303283065077\n",
            "At epoch 220, test loss: 3.1053303283065077\n",
            "At epoch 221, test loss: 3.1053303283065077\n",
            "At epoch 222, test loss: 3.105335096666721\n",
            "At epoch 223, test loss: 3.121854767972735\n",
            "At epoch 224, test loss: 3.1223573472361466\n",
            "At epoch 225, test loss: 3.1243365264508327\n",
            "At epoch 226, test loss: 3.1243370032878772\n",
            "At epoch 227, test loss: 3.1243370032878772\n",
            "At epoch 228, test loss: 3.1243370032878772\n",
            "At epoch 229, test loss: 3.124338910634691\n",
            "At epoch 230, test loss: 3.124338910634691\n",
            "At epoch 231, test loss: 3.1243405795633805\n",
            "At epoch 232, test loss: 3.124341652446418\n",
            "At epoch 233, test loss: 3.124341652446418\n",
            "At epoch 234, test loss: 3.1243530964727313\n",
            "At epoch 235, test loss: 3.1243530964727313\n",
            "At epoch 236, test loss: 3.1243566727450514\n",
            "At epoch 237, test loss: 3.1243988719422475\n",
            "At epoch 238, test loss: 3.124965551519125\n",
            "At epoch 239, test loss: 3.1279682125666284\n",
            "At epoch 240, test loss: 3.1279682125666284\n",
            "At epoch 241, test loss: 3.127968331775911\n",
            "At epoch 242, test loss: 3.127968331775911\n",
            "At epoch 243, test loss: 3.127968331775911\n",
            "At epoch 244, test loss: 3.127968331775911\n",
            "At epoch 245, test loss: 3.1284715067947317\n",
            "At epoch 246, test loss: 3.1284715067947317\n",
            "At epoch 247, test loss: 3.1284715067947317\n",
            "At epoch 248, test loss: 3.1284715067947317\n",
            "At epoch 249, test loss: 3.1284837852760674\n",
            "At epoch 250, test loss: 3.1288532657960886\n",
            "At epoch 251, test loss: 3.15235025681951\n",
            "At epoch 252, test loss: 3.1523645618323926\n",
            "At epoch 253, test loss: 3.1523645618323926\n",
            "At epoch 254, test loss: 3.1523645618323926\n",
            "At epoch 255, test loss: 3.15742492632463\n",
            "At epoch 256, test loss: 3.1574330325235778\n",
            "At epoch 257, test loss: 3.1574330325235778\n",
            "At epoch 258, test loss: 3.1574330325235778\n",
            "At epoch 259, test loss: 3.1574330325235778\n",
            "At epoch 260, test loss: 3.1574330325235778\n",
            "At epoch 261, test loss: 3.1574330325235778\n",
            "At epoch 262, test loss: 3.1574332709421284\n",
            "At epoch 263, test loss: 3.1574332709421284\n",
            "At epoch 264, test loss: 3.5482662911523946\n",
            "At epoch 265, test loss: 3.548266410361677\n",
            "At epoch 266, test loss: 3.548266410361677\n",
            "At epoch 267, test loss: 3.548266410361677\n",
            "At epoch 268, test loss: 3.5485875086157392\n",
            "At epoch 269, test loss: 3.5485875086157392\n",
            "At epoch 270, test loss: 3.5485875086157392\n",
            "At epoch 271, test loss: 3.5485876278250217\n",
            "At epoch 272, test loss: 3.5487325758146326\n",
            "At epoch 273, test loss: 3.5487325758146326\n",
            "At epoch 274, test loss: 3.581032645742802\n",
            "At epoch 275, test loss: 3.581032645742802\n",
            "At epoch 276, test loss: 3.581032645742802\n",
            "At epoch 277, test loss: 3.581032645742802\n",
            "At epoch 278, test loss: 3.581032645742802\n",
            "At epoch 279, test loss: 3.581032645742802\n",
            "At epoch 280, test loss: 3.5973560110138934\n",
            "At epoch 281, test loss: 3.609829675260393\n",
            "At epoch 282, test loss: 3.6100298076341133\n",
            "At epoch 283, test loss: 3.6100299268433957\n",
            "At epoch 284, test loss: 3.6100304036804403\n",
            "At epoch 285, test loss: 3.6100304036804403\n",
            "At epoch 286, test loss: 3.6100304036804403\n",
            "At epoch 287, test loss: 3.708460858411428\n",
            "At epoch 288, test loss: 3.7084618120852895\n",
            "At epoch 289, test loss: 3.7084624081315667\n",
            "At epoch 290, test loss: 3.7084624081315667\n",
            "At epoch 291, test loss: 4.001710674113241\n",
            "At epoch 292, test loss: 4.0017116277871025\n",
            "At epoch 293, test loss: 4.0017116277871025\n",
            "At epoch 294, test loss: 4.0017116277871025\n",
            "At epoch 295, test loss: 4.0017116277871025\n",
            "At epoch 296, test loss: 4.005589131351684\n",
            "At epoch 297, test loss: 4.005589131351684\n",
            "At epoch 298, test loss: 4.005589131351684\n",
            "At epoch 299, test loss: 4.00636869456352\n",
            "At epoch 300, test loss: 4.007556267284912\n",
            "At epoch 301, test loss: 4.007556267284912\n",
            "At epoch 302, test loss: 4.007556267284912\n",
            "At epoch 303, test loss: 4.007592029431493\n",
            "At epoch 304, test loss: 4.007687869109162\n",
            "At epoch 305, test loss: 4.007698597887924\n",
            "At epoch 306, test loss: 4.013835576398691\n",
            "At epoch 307, test loss: 4.013835576398691\n",
            "At epoch 308, test loss: 4.013838318208485\n",
            "At epoch 309, test loss: 4.01383879504553\n",
            "At epoch 310, test loss: 4.01383879504553\n",
            "At epoch 311, test loss: 4.014464328912226\n",
            "At epoch 312, test loss: 4.014464567330776\n",
            "At epoch 313, test loss: 4.014464567330776\n",
            "At epoch 314, test loss: 4.014464567330776\n",
            "At epoch 315, test loss: 4.014464567330776\n",
            "At epoch 316, test loss: 4.014464805749327\n",
            "At epoch 317, test loss: 4.014465044167878\n",
            "At epoch 318, test loss: 4.01469795199489\n",
            "At epoch 319, test loss: 4.01469795199489\n",
            "At epoch 320, test loss: 4.01469795199489\n",
            "At epoch 321, test loss: 4.014698190413441\n",
            "At epoch 322, test loss: 4.014699263296478\n",
            "At epoch 323, test loss: 4.014699263296478\n",
            "At epoch 324, test loss: 4.014699263296478\n",
            "At epoch 325, test loss: 4.014699263296478\n",
            "At epoch 326, test loss: 4.014699263296478\n",
            "At epoch 327, test loss: 4.014699263296478\n",
            "At epoch 328, test loss: 4.014699263296478\n",
            "At epoch 329, test loss: 4.01470069380693\n",
            "At epoch 330, test loss: 4.01470069380693\n",
            "At epoch 331, test loss: 4.033707555180712\n",
            "At epoch 332, test loss: 4.033707555180712\n",
            "At epoch 333, test loss: 4.033762032341741\n",
            "At epoch 334, test loss: 4.033762032341741\n",
            "At epoch 335, test loss: 4.033762032341741\n",
            "At epoch 336, test loss: 4.033762032341741\n",
            "At epoch 337, test loss: 4.033762032341741\n",
            "At epoch 338, test loss: 4.033773118743824\n",
            "At epoch 339, test loss: 4.033865024886012\n",
            "At epoch 340, test loss: 4.033865024886012\n",
            "At epoch 341, test loss: 4.033865024886012\n",
            "At epoch 342, test loss: 4.033865024886012\n",
            "At epoch 343, test loss: 4.033865024886012\n",
            "At epoch 344, test loss: 4.033865144095294\n",
            "At epoch 345, test loss: 4.033865144095294\n",
            "At epoch 346, test loss: 4.033865144095294\n",
            "At epoch 347, test loss: 4.033865144095294\n",
            "At epoch 348, test loss: 4.033865144095294\n",
            "At epoch 349, test loss: 4.03403369182805\n",
            "At epoch 350, test loss: 4.034066354638604\n",
            "At epoch 351, test loss: 4.034066354638604\n",
            "At epoch 352, test loss: 4.034084116664495\n",
            "At epoch 353, test loss: 4.036137413789298\n",
            "At epoch 354, test loss: 4.036137413789298\n",
            "At epoch 355, test loss: 4.076608203907277\n",
            "At epoch 356, test loss: 4.0766142835623995\n",
            "At epoch 357, test loss: 4.076616071700151\n",
            "At epoch 358, test loss: 4.076616071700151\n",
            "At epoch 359, test loss: 4.076616071700151\n",
            "At epoch 360, test loss: 4.076616190909434\n",
            "At epoch 361, test loss: 4.076616190909434\n",
            "At epoch 362, test loss: 4.076616190909434\n",
            "At epoch 363, test loss: 4.076616190909434\n",
            "At epoch 364, test loss: 4.076616190909434\n",
            "At epoch 365, test loss: 4.076617263792471\n",
            "At epoch 366, test loss: 4.076617263792471\n",
            "At epoch 367, test loss: 4.076617263792471\n",
            "At epoch 368, test loss: 4.076617263792471\n",
            "At epoch 369, test loss: 4.076617263792471\n",
            "At epoch 370, test loss: 4.076617263792471\n",
            "At epoch 371, test loss: 4.250821732519583\n",
            "At epoch 372, test loss: 4.250822090147395\n",
            "At epoch 373, test loss: 4.250842951554787\n",
            "At epoch 374, test loss: 4.250842951554787\n",
            "At epoch 375, test loss: 4.250844739692539\n",
            "At epoch 376, test loss: 4.250844739692539\n",
            "At epoch 377, test loss: 4.2533277641872616\n",
            "At epoch 378, test loss: 4.2533277641872616\n",
            "At epoch 379, test loss: 4.2533277641872616\n",
            "At epoch 380, test loss: 4.253328121815073\n",
            "At epoch 381, test loss: 4.253328837070583\n",
            "At epoch 382, test loss: 4.304704494390606\n",
            "At epoch 383, test loss: 4.304704613599888\n",
            "At epoch 384, test loss: 4.304704613599888\n",
            "At epoch 385, test loss: 4.304704613599888\n",
            "At epoch 386, test loss: 4.315515585791282\n",
            "At epoch 387, test loss: 4.315515585791282\n",
            "At epoch 388, test loss: 4.315515705000564\n",
            "At epoch 389, test loss: 4.315563625985362\n",
            "At epoch 390, test loss: 4.315563625985362\n",
            "At epoch 391, test loss: 4.3222113594911775\n",
            "At epoch 392, test loss: 4.322216723894655\n",
            "At epoch 393, test loss: 4.322216723894655\n",
            "At epoch 394, test loss: 4.323410607257813\n",
            "At epoch 395, test loss: 4.3234535216816425\n",
            "At epoch 396, test loss: 4.3234535216816425\n",
            "At epoch 397, test loss: 4.448746657324648\n",
            "At epoch 398, test loss: 4.44874677653393\n",
            "At epoch 399, test loss: 4.44874677653393\n",
            "At epoch 400, test loss: 4.448747134161742\n",
            "At epoch 401, test loss: 4.455728162871303\n",
            "At epoch 402, test loss: 4.455728162871303\n",
            "At epoch 403, test loss: 4.455728162871303\n",
            "At epoch 404, test loss: 4.455728162871303\n",
            "At epoch 405, test loss: 4.455730308636241\n",
            "At epoch 406, test loss: 4.455730308636241\n",
            "At epoch 407, test loss: 4.45823235911984\n",
            "At epoch 408, test loss: 5.145114080139585\n",
            "At epoch 409, test loss: 5.145114437767397\n",
            "At epoch 410, test loss: 5.328647554942499\n",
            "At epoch 411, test loss: 5.328648150988776\n",
            "At epoch 412, test loss: 5.328648150988776\n",
            "At epoch 413, test loss: 5.328652204096436\n",
            "At epoch 414, test loss: 5.328652204096436\n",
            "At epoch 415, test loss: 5.328661860002505\n",
            "At epoch 416, test loss: 5.328661860002505\n",
            "At epoch 417, test loss: 5.328661860002505\n",
            "At epoch 418, test loss: 5.328670919867584\n",
            "At epoch 419, test loss: 5.328670919867584\n",
            "At epoch 420, test loss: 5.328719317667712\n",
            "At epoch 421, test loss: 5.328719317667712\n",
            "At epoch 422, test loss: 5.328719317667712\n",
            "At epoch 423, test loss: 5.328719675295524\n",
            "At epoch 424, test loss: 5.328719675295524\n",
            "At epoch 425, test loss: 5.328720867387737\n",
            "At epoch 426, test loss: 5.328720867387737\n",
            "At epoch 427, test loss: 5.328781066267247\n",
            "At epoch 428, test loss: 5.328781066267247\n",
            "At epoch 429, test loss: 7.231676755277867\n",
            "At epoch 430, test loss: 7.241831954373474\n",
            "At epoch 431, test loss: 7.242317256817323\n",
            "At epoch 432, test loss: 7.242317256817323\n",
            "At epoch 433, test loss: 7.2423174952358735\n",
            "At epoch 434, test loss: 7.2423174952358735\n",
            "At epoch 435, test loss: 7.2423174952358735\n",
            "At epoch 436, test loss: 7.2423174952358735\n",
            "At epoch 437, test loss: 7.2423174952358735\n",
            "At epoch 438, test loss: 7.2423174952358735\n",
            "At epoch 439, test loss: 7.2423174952358735\n",
            "At epoch 440, test loss: 7.2423174952358735\n",
            "At epoch 441, test loss: 7.242319044955387\n",
            "At epoch 442, test loss: 7.24231916416467\n",
            "At epoch 443, test loss: 7.24231916416467\n",
            "At epoch 444, test loss: 7.24231916416467\n",
            "At epoch 445, test loss: 7.24231916416467\n",
            "At epoch 446, test loss: 7.24231916416467\n",
            "At epoch 447, test loss: 7.24231916416467\n",
            "At epoch 448, test loss: 7.24231916416467\n",
            "At epoch 449, test loss: 7.242320117838531\n",
            "At epoch 450, test loss: 7.242378767088226\n",
            "At epoch 451, test loss: 7.242378767088226\n",
            "At epoch 452, test loss: 7.2494656219819404\n",
            "At epoch 453, test loss: 7.2494656219819404\n",
            "At epoch 454, test loss: 7.2494656219819404\n",
            "At epoch 455, test loss: 7.2494656219819404\n",
            "At epoch 456, test loss: 7.249471463219848\n",
            "At epoch 457, test loss: 7.2588048930779365\n",
            "At epoch 458, test loss: 7.258807038842875\n",
            "At epoch 459, test loss: 7.45348765510245\n",
            "At epoch 460, test loss: 7.45348765510245\n",
            "At epoch 461, test loss: 7.455128773302945\n",
            "At epoch 462, test loss: 7.455335818403924\n",
            "At epoch 463, test loss: 7.4553359376132065\n",
            "At epoch 464, test loss: 7.4553359376132065\n",
            "At epoch 465, test loss: 7.4553359376132065\n",
            "At epoch 466, test loss: 7.455336176031757\n",
            "At epoch 467, test loss: 7.455337010496443\n",
            "At epoch 468, test loss: 7.455337010496443\n",
            "At epoch 469, test loss: 7.455591370756885\n",
            "At epoch 470, test loss: 7.455591370756885\n",
            "At epoch 471, test loss: 7.455598880913932\n",
            "At epoch 472, test loss: 7.455598880913932\n",
            "At epoch 473, test loss: 7.455642272153092\n",
            "At epoch 474, test loss: 7.4561179233011\n",
            "At epoch 475, test loss: 7.4561179233011\n",
            "At epoch 476, test loss: 7.4561179233011\n",
            "At epoch 477, test loss: 7.4561179233011\n",
            "At epoch 478, test loss: 7.4561179233011\n",
            "At epoch 479, test loss: 7.4561179233011\n",
            "At epoch 480, test loss: 7.4561179233011\n",
            "At epoch 481, test loss: 7.456121022737854\n",
            "At epoch 482, test loss: 7.492520487792127\n",
            "At epoch 483, test loss: 7.492520487792127\n",
            "At epoch 484, test loss: 7.492523468019819\n",
            "At epoch 485, test loss: 7.492523468019819\n",
            "At epoch 486, test loss: 7.492523468019819\n",
            "At epoch 487, test loss: 7.532272881748533\n",
            "At epoch 488, test loss: 7.532272881748533\n",
            "At epoch 489, test loss: 7.554971001031255\n",
            "At epoch 490, test loss: 7.554971001031255\n",
            "At epoch 491, test loss: 7.554971001031255\n",
            "At epoch 492, test loss: 7.554971001031255\n",
            "At epoch 493, test loss: 7.554971001031255\n",
            "At epoch 494, test loss: 7.554971001031255\n",
            "At epoch 495, test loss: 7.565194286326324\n",
            "At epoch 496, test loss: 7.565194286326324\n",
            "At epoch 497, test loss: 7.565194286326324\n",
            "At epoch 498, test loss: 7.565194286326324\n",
            "At epoch 499, test loss: 7.565195359209362\n",
            "At epoch 500, test loss: 7.56520060440446\n",
            "At epoch 501, test loss: 7.565202273333149\n",
            "At epoch 502, test loss: 7.565308840762761\n",
            "At epoch 503, test loss: 7.565313847540189\n",
            "At epoch 504, test loss: 7.565313847540189\n",
            "At epoch 505, test loss: 7.565313847540189\n",
            "At epoch 506, test loss: 7.565329225420307\n",
            "At epoch 507, test loss: 7.565329225420307\n",
            "At epoch 508, test loss: 7.56542387311422\n",
            "At epoch 509, test loss: 7.56542458836973\n",
            "At epoch 510, test loss: 7.56542458836973\n",
            "At epoch 511, test loss: 7.566243816551811\n",
            "At epoch 512, test loss: 7.566245366271325\n",
            "At epoch 513, test loss: 7.566245366271325\n",
            "At epoch 514, test loss: 7.56663141062527\n",
            "At epoch 515, test loss: 7.711922694351962\n",
            "At epoch 516, test loss: 7.711923767235\n",
            "At epoch 517, test loss: 7.711923767235\n",
            "At epoch 518, test loss: 7.759927250798661\n",
            "At epoch 519, test loss: 7.759956814265912\n",
            "At epoch 520, test loss: 7.759956814265912\n",
            "At epoch 521, test loss: 7.759957529521422\n",
            "At epoch 522, test loss: 7.759957529521422\n",
            "At epoch 523, test loss: 7.759957529521422\n",
            "At epoch 524, test loss: 7.759957529521422\n",
            "At epoch 525, test loss: 7.759957529521422\n",
            "At epoch 526, test loss: 7.759957529521422\n",
            "At epoch 527, test loss: 7.759957529521422\n",
            "At epoch 528, test loss: 7.759957529521422\n",
            "At epoch 529, test loss: 7.759959913704485\n",
            "At epoch 530, test loss: 7.759959913704485\n",
            "At epoch 531, test loss: 7.759959913704485\n",
            "At epoch 532, test loss: 7.759959913704485\n",
            "At epoch 533, test loss: 7.759959913704485\n",
            "At epoch 534, test loss: 7.759966470194037\n",
            "At epoch 535, test loss: 7.7600667201788625\n",
            "At epoch 536, test loss: 7.76007315745958\n",
            "At epoch 537, test loss: 7.76007315745958\n",
            "At epoch 538, test loss: 7.760659495218739\n",
            "At epoch 539, test loss: 7.760659495218739\n",
            "At epoch 540, test loss: 7.760660448892601\n",
            "At epoch 541, test loss: 7.760665217252814\n",
            "At epoch 542, test loss: 7.760674038701133\n",
            "At epoch 543, test loss: 7.760699191544724\n",
            "At epoch 544, test loss: 7.760699191544724\n",
            "At epoch 545, test loss: 7.760699668381768\n",
            "At epoch 546, test loss: 7.760699668381768\n",
            "At epoch 547, test loss: 7.762575216306779\n",
            "At epoch 548, test loss: 7.762575216306779\n",
            "At epoch 549, test loss: 7.762619441974586\n",
            "At epoch 550, test loss: 7.789629215974038\n",
            "At epoch 551, test loss: 7.792531679751029\n",
            "At epoch 552, test loss: 7.792531679751029\n",
            "At epoch 553, test loss: 7.792531679751029\n",
            "At epoch 554, test loss: 7.792620486728175\n",
            "At epoch 555, test loss: 7.792620486728175\n",
            "At epoch 556, test loss: 7.792620486728175\n",
            "At epoch 557, test loss: 7.836024440042003\n",
            "At epoch 558, test loss: 7.836024440042003\n",
            "At epoch 559, test loss: 7.836024440042003\n",
            "At epoch 560, test loss: 7.8360255129250405\n",
            "At epoch 561, test loss: 7.8360255129250405\n",
            "At epoch 562, test loss: 7.8360255129250405\n",
            "At epoch 563, test loss: 7.8360255129250405\n",
            "At epoch 564, test loss: 7.8360255129250405\n",
            "At epoch 565, test loss: 7.8364736394164325\n",
            "At epoch 566, test loss: 7.8364736394164325\n",
            "At epoch 567, test loss: 7.842330373651272\n",
            "At epoch 568, test loss: 7.923400685018784\n",
            "At epoch 569, test loss: 7.923652661704615\n",
            "At epoch 570, test loss: 7.923652661704615\n",
            "At epoch 571, test loss: 7.923652661704615\n",
            "At epoch 572, test loss: 7.923652661704615\n",
            "At epoch 573, test loss: 7.923652661704615\n",
            "At epoch 574, test loss: 7.923652661704615\n",
            "At epoch 575, test loss: 7.923652900123166\n",
            "At epoch 576, test loss: 7.923652900123166\n",
            "At epoch 577, test loss: 7.933748032638\n",
            "At epoch 578, test loss: 7.933748151847283\n",
            "At epoch 579, test loss: 7.933748151847283\n",
            "At epoch 580, test loss: 7.93375387387681\n",
            "At epoch 581, test loss: 11.678248788885355\n",
            "At epoch 582, test loss: 11.678248788885355\n",
            "At epoch 583, test loss: 11.678248788885355\n",
            "At epoch 584, test loss: 11.678248788885355\n",
            "At epoch 585, test loss: 11.678248788885355\n",
            "At epoch 586, test loss: 11.6782492657224\n",
            "At epoch 587, test loss: 12.138959672025806\n",
            "At epoch 588, test loss: 12.138963248298126\n",
            "At epoch 589, test loss: 12.138963248298126\n",
            "At epoch 590, test loss: 12.138963248298126\n",
            "At epoch 591, test loss: 12.13913334548971\n",
            "At epoch 592, test loss: 12.13913334548971\n",
            "At epoch 593, test loss: 12.13913334548971\n",
            "At epoch 594, test loss: 12.139133703117523\n",
            "At epoch 595, test loss: 12.139133703117523\n",
            "At epoch 596, test loss: 12.139133703117523\n",
            "At epoch 597, test loss: 12.166262982567105\n",
            "At epoch 598, test loss: 12.166262982567105\n",
            "At epoch 599, test loss: 12.16626429386838\n",
            "At epoch 600, test loss: 12.16626429386838\n",
            "At epoch 601, test loss: 12.16626429386838\n",
            "At epoch 602, test loss: 12.166294810980844\n",
            "At epoch 603, test loss: 12.166294810980844\n",
            "At epoch 604, test loss: 12.166294810980844\n",
            "At epoch 605, test loss: 12.877731440276193\n",
            "At epoch 606, test loss: 12.877731440276193\n",
            "At epoch 607, test loss: 12.877731678694744\n",
            "At epoch 608, test loss: 12.877767083228008\n",
            "At epoch 609, test loss: 12.877767083228008\n",
            "At epoch 610, test loss: 12.877789017497179\n",
            "At epoch 611, test loss: 12.877796050820706\n",
            "At epoch 612, test loss: 13.33618160775697\n",
            "At epoch 613, test loss: 13.3361973432595\n",
            "At epoch 614, test loss: 13.336267674266509\n",
            "At epoch 615, test loss: 13.33626791268506\n",
            "At epoch 616, test loss: 13.336269223986335\n",
            "At epoch 617, test loss: 13.343619530478634\n",
            "At epoch 618, test loss: 13.359513913670696\n",
            "At epoch 619, test loss: 13.359514032879979\n",
            "At epoch 620, test loss: 13.359514509717023\n",
            "At epoch 621, test loss: 13.359514509717023\n",
            "At epoch 622, test loss: 13.813749676028287\n",
            "At epoch 623, test loss: 13.81374979523757\n",
            "At epoch 624, test loss: 13.813749914446852\n",
            "At epoch 625, test loss: 13.813749914446852\n",
            "At epoch 626, test loss: 20.984629755297682\n",
            "At epoch 627, test loss: 20.984629874506965\n",
            "At epoch 628, test loss: 20.984629874506965\n",
            "At epoch 629, test loss: 20.984629874506965\n",
            "At epoch 630, test loss: 20.984629874506965\n",
            "At epoch 631, test loss: 20.984629874506965\n",
            "At epoch 632, test loss: 20.984629874506965\n",
            "At epoch 633, test loss: 20.984640007245645\n",
            "At epoch 634, test loss: 20.984640007245645\n",
            "At epoch 635, test loss: 20.98464346430913\n",
            "At epoch 636, test loss: 20.993062721897815\n",
            "At epoch 637, test loss: 20.993062721897815\n",
            "At epoch 638, test loss: 22.406878339459155\n",
            "At epoch 639, test loss: 22.406878339459155\n",
            "At epoch 640, test loss: 22.41499209623501\n",
            "At epoch 641, test loss: 22.414992215444293\n",
            "At epoch 642, test loss: 22.414992215444293\n",
            "At epoch 643, test loss: 22.414992215444293\n",
            "At epoch 644, test loss: 22.41499281149057\n",
            "At epoch 645, test loss: 22.943510117341553\n",
            "At epoch 646, test loss: 22.943510594178598\n",
            "At epoch 647, test loss: 22.943510594178598\n",
            "At epoch 648, test loss: 22.943510594178598\n",
            "At epoch 649, test loss: 22.943555415867387\n",
            "At epoch 650, test loss: 22.943555415867387\n",
            "At epoch 651, test loss: 22.943583191246226\n",
            "At epoch 652, test loss: 23.00823854735537\n",
            "At epoch 653, test loss: 23.00823854735537\n",
            "At epoch 654, test loss: 23.008239381820054\n",
            "At epoch 655, test loss: 23.08418203225741\n",
            "At epoch 656, test loss: 23.084559735162507\n",
            "At epoch 657, test loss: 23.084808255652057\n",
            "At epoch 658, test loss: 23.084808255652057\n",
            "At epoch 659, test loss: 23.084808255652057\n",
            "At epoch 660, test loss: 23.10795143722938\n",
            "At epoch 661, test loss: 23.107951556438664\n",
            "At epoch 662, test loss: 23.107951556438664\n",
            "At epoch 663, test loss: 23.30784214019225\n",
            "At epoch 664, test loss: 23.30784214019225\n",
            "At epoch 665, test loss: 23.307848458264587\n",
            "At epoch 666, test loss: 23.307848458264587\n",
            "At epoch 667, test loss: 23.307848458264587\n",
            "At epoch 668, test loss: 23.307848458264587\n",
            "At epoch 669, test loss: 23.307848458264587\n",
            "At epoch 670, test loss: 23.30794167558527\n",
            "At epoch 671, test loss: 23.30794179479455\n",
            "At epoch 672, test loss: 23.30794179479455\n",
            "At epoch 673, test loss: 23.30794179479455\n",
            "At epoch 674, test loss: 23.308038111258448\n",
            "At epoch 675, test loss: 23.30804407170519\n",
            "At epoch 676, test loss: 23.30804407170519\n",
            "At epoch 677, test loss: 23.308045144588228\n",
            "At epoch 678, test loss: 23.308045144588228\n",
            "At epoch 679, test loss: 23.308252785604687\n",
            "At epoch 680, test loss: 23.308252785604687\n",
            "At epoch 681, test loss: 23.308252785604687\n",
            "At epoch 682, test loss: 45.27143714229414\n",
            "At epoch 683, test loss: 45.27143738071269\n",
            "At epoch 684, test loss: 45.27143749992197\n",
            "At epoch 685, test loss: 45.27162964683035\n",
            "At epoch 686, test loss: 45.27162964683035\n",
            "At epoch 687, test loss: 45.27162964683035\n",
            "At epoch 688, test loss: 45.27165193871829\n",
            "At epoch 689, test loss: 45.27179295336887\n",
            "At epoch 690, test loss: 45.27181476843057\n",
            "At epoch 691, test loss: 45.273184498301504\n",
            "At epoch 692, test loss: 45.273184498301504\n",
            "At epoch 693, test loss: 45.299723997241614\n",
            "At epoch 694, test loss: 45.299723997241614\n",
            "At epoch 695, test loss: 45.299723997241614\n",
            "At epoch 696, test loss: 45.299723997241614\n",
            "At epoch 697, test loss: 45.299724116450896\n",
            "At epoch 698, test loss: 45.299724116450896\n",
            "At epoch 699, test loss: 45.30146401130573\n",
            "At epoch 700, test loss: 45.30146401130573\n",
            "At epoch 701, test loss: 45.30146401130573\n",
            "At epoch 702, test loss: 45.301604429982625\n",
            "At epoch 703, test loss: 45.30399327029104\n",
            "At epoch 704, test loss: 45.30399327029104\n",
            "At epoch 705, test loss: 47.02852968920583\n",
            "At epoch 706, test loss: 47.02852968920583\n",
            "At epoch 707, test loss: 47.02852968920583\n",
            "At epoch 708, test loss: 47.02852968920583\n",
            "At epoch 709, test loss: 47.02852968920583\n",
            "At epoch 710, test loss: 47.02852968920583\n",
            "At epoch 711, test loss: 47.02852968920583\n",
            "At epoch 712, test loss: 47.02852968920583\n",
            "At epoch 713, test loss: 47.02962212220395\n",
            "At epoch 714, test loss: 47.10279600278104\n",
            "At epoch 715, test loss: 47.103919276875466\n",
            "At epoch 716, test loss: 47.103919276875466\n",
            "At epoch 717, test loss: 47.108246377577096\n",
            "At epoch 718, test loss: 47.108246377577096\n",
            "At epoch 719, test loss: 47.108246377577096\n",
            "At epoch 720, test loss: 47.108246377577096\n",
            "At epoch 721, test loss: 47.11137216156444\n",
            "At epoch 722, test loss: 47.11137216156444\n",
            "At epoch 723, test loss: 47.11137216156444\n",
            "At epoch 724, test loss: 47.11137228077372\n",
            "At epoch 725, test loss: 54.00075368946513\n",
            "At epoch 726, test loss: 54.00075368946513\n",
            "At epoch 727, test loss: 54.00075368946513\n",
            "At epoch 728, test loss: 54.00075368946513\n",
            "At epoch 729, test loss: 54.00080947785558\n",
            "At epoch 730, test loss: 54.00080947785558\n",
            "At epoch 731, test loss: 54.00080947785558\n",
            "At epoch 732, test loss: 54.000809597064865\n",
            "At epoch 733, test loss: 54.00086264379082\n",
            "At epoch 734, test loss: 54.00086264379082\n",
            "At epoch 735, test loss: 54.00086264379082\n",
            "At epoch 736, test loss: 62.39173846013603\n",
            "At epoch 737, test loss: 62.39175336118627\n",
            "At epoch 738, test loss: 62.39428121512595\n",
            "At epoch 739, test loss: 62.39428121512595\n",
            "At epoch 740, test loss: 62.39428121512595\n",
            "At epoch 741, test loss: 62.408381194899114\n",
            "At epoch 742, test loss: 62.408381194899114\n",
            "At epoch 743, test loss: 62.40838369829101\n",
            "At epoch 744, test loss: 62.40838369829101\n",
            "At epoch 745, test loss: 62.40838369829101\n",
            "At epoch 746, test loss: 62.40838369829101\n",
            "At epoch 747, test loss: 62.40838369829101\n",
            "At epoch 748, test loss: 62.40838369829101\n",
            "At epoch 749, test loss: 62.40838369829101\n",
            "At epoch 750, test loss: 62.40838500959229\n",
            "At epoch 751, test loss: 62.41035026879411\n",
            "At epoch 752, test loss: 62.689042454672865\n",
            "At epoch 753, test loss: 62.689042454672865\n",
            "At epoch 754, test loss: 62.689042454672865\n",
            "At epoch 755, test loss: 62.689042454672865\n",
            "At epoch 756, test loss: 62.689042454672865\n",
            "At epoch 757, test loss: 62.69109967755515\n",
            "At epoch 758, test loss: 62.69109967755515\n",
            "At epoch 759, test loss: 62.691564843204226\n",
            "At epoch 760, test loss: 62.69156508162278\n",
            "At epoch 761, test loss: 62.692942549503734\n",
            "At epoch 762, test loss: 62.69311860612661\n",
            "At epoch 763, test loss: 62.69312432815614\n",
            "At epoch 764, test loss: 62.69312432815614\n",
            "At epoch 765, test loss: 62.69312480499318\n",
            "At epoch 766, test loss: 62.693136010603645\n",
            "At epoch 767, test loss: 62.693136725859155\n",
            "At epoch 768, test loss: 62.693136725859155\n",
            "At epoch 769, test loss: 62.693136725859155\n",
            "At epoch 770, test loss: 62.693136725859155\n",
            "At epoch 771, test loss: 62.693136725859155\n",
            "At epoch 772, test loss: 62.693136725859155\n",
            "At epoch 773, test loss: 62.693136725859155\n",
            "At epoch 774, test loss: 62.69313684506844\n",
            "At epoch 775, test loss: 62.69313815636971\n",
            "At epoch 776, test loss: 62.69313815636971\n",
            "At epoch 777, test loss: 62.69313815636971\n",
            "At epoch 778, test loss: 62.69314566652676\n",
            "At epoch 779, test loss: 62.69318941537924\n",
            "At epoch 780, test loss: 62.69326272640748\n",
            "At epoch 781, test loss: 62.69326272640748\n",
            "At epoch 782, test loss: 62.6933324614117\n",
            "At epoch 783, test loss: 62.6933324614117\n",
            "At epoch 784, test loss: 62.707243418092474\n",
            "At epoch 785, test loss: 62.70724532543929\n",
            "At epoch 786, test loss: 62.71107770363116\n",
            "At epoch 787, test loss: 62.71107770363116\n",
            "At epoch 788, test loss: 62.72929614642882\n",
            "At epoch 789, test loss: 62.72929614642882\n",
            "At epoch 790, test loss: 62.73257022840218\n",
            "At epoch 791, test loss: 62.73257356625683\n",
            "At epoch 792, test loss: 62.739838548234474\n",
            "At epoch 793, test loss: 62.745349282628666\n",
            "At epoch 794, test loss: 62.74535369336283\n",
            "At epoch 795, test loss: 62.74535405099064\n",
            "At epoch 796, test loss: 63.72161185204655\n",
            "At epoch 797, test loss: 63.736218301799596\n",
            "At epoch 798, test loss: 63.736218301799596\n",
            "At epoch 799, test loss: 63.736265865174715\n",
            "At epoch 800, test loss: 63.736265865174715\n",
            "At epoch 801, test loss: 63.736349784992996\n",
            "At epoch 802, test loss: 63.736349784992996\n",
            "At epoch 803, test loss: 63.73635014262081\n",
            "At epoch 804, test loss: 63.73635014262081\n",
            "At epoch 805, test loss: 63.73635026183009\n",
            "At epoch 806, test loss: 63.73635026183009\n",
            "At epoch 807, test loss: 63.73635026183009\n",
            "At epoch 808, test loss: 63.73636027536039\n",
            "At epoch 809, test loss: 63.73636170587084\n",
            "At epoch 810, test loss: 63.73636170587084\n",
            "At epoch 811, test loss: 63.73636170587084\n",
            "At epoch 812, test loss: 63.73636170587084\n",
            "At epoch 813, test loss: 63.73636170587084\n",
            "At epoch 814, test loss: 63.73636170587084\n",
            "At epoch 815, test loss: 63.73636373242672\n",
            "At epoch 816, test loss: 63.73636373242672\n",
            "At epoch 817, test loss: 63.74579364637221\n",
            "At epoch 818, test loss: 63.74592226493015\n",
            "At epoch 819, test loss: 63.745975788467796\n",
            "At epoch 820, test loss: 63.74599295445815\n",
            "At epoch 821, test loss: 63.7459934312952\n",
            "At epoch 822, test loss: 63.7459934312952\n",
            "At epoch 823, test loss: 63.90880378672008\n",
            "At epoch 824, test loss: 63.90880378672008\n",
            "At epoch 825, test loss: 63.908954098341916\n",
            "At epoch 826, test loss: 63.90895588647967\n",
            "At epoch 827, test loss: 64.75376243841296\n",
            "At epoch 828, test loss: 64.75376255762225\n",
            "At epoch 829, test loss: 64.75376255762225\n",
            "At epoch 830, test loss: 64.75644284042923\n",
            "At epoch 831, test loss: 64.75644284042923\n",
            "At epoch 832, test loss: 64.75644284042923\n",
            "At epoch 833, test loss: 64.75644820483271\n",
            "At epoch 834, test loss: 64.7638220598163\n",
            "At epoch 835, test loss: 64.81931296709794\n",
            "At epoch 836, test loss: 64.81985653293654\n",
            "At epoch 837, test loss: 64.81985736740123\n",
            "At epoch 838, test loss: 64.81990421555335\n",
            "At epoch 839, test loss: 64.81990552685463\n",
            "At epoch 840, test loss: 64.82027810568113\n",
            "At epoch 841, test loss: 64.82041506776883\n",
            "At epoch 842, test loss: 64.82041900166766\n",
            "At epoch 843, test loss: 64.82041900166766\n",
            "At epoch 844, test loss: 64.82041900166766\n",
            "At epoch 845, test loss: 64.86782579058305\n",
            "At epoch 846, test loss: 64.90109169044987\n",
            "At epoch 847, test loss: 64.90109169044987\n",
            "At epoch 848, test loss: 64.90110468417785\n",
            "At epoch 849, test loss: 64.90112959860832\n",
            "At epoch 850, test loss: 64.90112983702687\n",
            "At epoch 851, test loss: 64.90112983702687\n",
            "At epoch 852, test loss: 64.90112983702687\n",
            "At epoch 853, test loss: 64.90112983702687\n",
            "At epoch 854, test loss: 68.7113672447112\n",
            "At epoch 855, test loss: 68.71139132469847\n",
            "At epoch 856, test loss: 68.71147095333157\n",
            "At epoch 857, test loss: 68.71147095333157\n",
            "At epoch 858, test loss: 68.71147095333157\n",
            "At epoch 859, test loss: 68.71999860382942\n",
            "At epoch 860, test loss: 68.71999860382942\n",
            "At epoch 861, test loss: 68.71999860382942\n",
            "At epoch 862, test loss: 68.71999860382942\n",
            "At epoch 863, test loss: 68.71999860382942\n",
            "At epoch 864, test loss: 68.72001445853942\n",
            "At epoch 865, test loss: 68.72015916813956\n",
            "At epoch 866, test loss: 68.72016000260425\n",
            "At epoch 867, test loss: 68.72031532024499\n",
            "At epoch 868, test loss: 68.72031532024499\n",
            "At epoch 869, test loss: 68.72031532024499\n",
            "At epoch 870, test loss: 68.72031532024499\n",
            "At epoch 871, test loss: 68.72032938684202\n",
            "At epoch 872, test loss: 68.72032938684202\n",
            "At epoch 873, test loss: 68.72032938684202\n",
            "At epoch 874, test loss: 68.72032938684202\n",
            "At epoch 875, test loss: 68.9215389021752\n",
            "At epoch 876, test loss: 68.9215389021752\n",
            "At epoch 877, test loss: 68.9215389021752\n",
            "At epoch 878, test loss: 68.9215389021752\n",
            "At epoch 879, test loss: 68.92154843887289\n",
            "At epoch 880, test loss: 68.92154843887289\n",
            "At epoch 881, test loss: 68.92154843887289\n",
            "At epoch 882, test loss: 68.9215542801108\n",
            "At epoch 883, test loss: 68.92155439932009\n",
            "At epoch 884, test loss: 68.92155678350315\n",
            "At epoch 885, test loss: 68.92155714113096\n",
            "At epoch 886, test loss: 68.92155714113096\n",
            "At epoch 887, test loss: 68.9711069591699\n",
            "At epoch 888, test loss: 68.9711069591699\n",
            "At epoch 889, test loss: 68.97124928493349\n",
            "At epoch 890, test loss: 68.97124940414278\n",
            "At epoch 891, test loss: 68.97125035781664\n",
            "At epoch 892, test loss: 68.97125035781664\n",
            "At epoch 893, test loss: 68.97341024178331\n",
            "At epoch 894, test loss: 68.97341024178331\n",
            "At epoch 895, test loss: 68.97425043339626\n",
            "At epoch 896, test loss: 68.97425043339626\n",
            "At epoch 897, test loss: 68.97425043339626\n",
            "At epoch 898, test loss: 68.97430538736899\n",
            "At epoch 899, test loss: 68.97430538736899\n",
            "At epoch 900, test loss: 68.97430538736899\n",
            "At epoch 901, test loss: 68.97522728455169\n",
            "At epoch 902, test loss: 68.97522728455169\n",
            "At epoch 903, test loss: 68.97522752297024\n",
            "At epoch 904, test loss: 68.97522752297024\n",
            "At epoch 905, test loss: 68.97530238360284\n",
            "At epoch 906, test loss: 68.97530238360284\n",
            "At epoch 907, test loss: 68.97530238360284\n",
            "At epoch 908, test loss: 68.97530238360284\n",
            "At epoch 909, test loss: 68.97530488699473\n",
            "At epoch 910, test loss: 68.97530488699473\n",
            "At epoch 911, test loss: 68.97530488699473\n",
            "At epoch 912, test loss: 68.97532372188422\n",
            "At epoch 913, test loss: 68.97532717894771\n",
            "At epoch 914, test loss: 68.97558952416418\n",
            "At epoch 915, test loss: 68.97558952416418\n",
            "At epoch 916, test loss: 68.97558952416418\n",
            "At epoch 917, test loss: 70.55939198578832\n",
            "At epoch 918, test loss: 70.55939198578832\n",
            "At epoch 919, test loss: 70.55939508522508\n",
            "At epoch 920, test loss: 70.55939508522508\n",
            "At epoch 921, test loss: 70.55939508522508\n",
            "At epoch 922, test loss: 70.55939508522508\n",
            "At epoch 923, test loss: 70.55939508522508\n",
            "At epoch 924, test loss: 70.55939508522508\n",
            "At epoch 925, test loss: 70.55939508522508\n",
            "At epoch 926, test loss: 70.55956804294485\n",
            "At epoch 927, test loss: 70.55956911582788\n",
            "At epoch 928, test loss: 70.56067309943782\n",
            "At epoch 929, test loss: 70.57709579584943\n",
            "At epoch 930, test loss: 70.57709579584943\n",
            "At epoch 931, test loss: 70.57800935606528\n",
            "At epoch 932, test loss: 70.57830912249219\n",
            "At epoch 933, test loss: 70.57890046413112\n",
            "At epoch 934, test loss: 70.5793810005026\n",
            "At epoch 935, test loss: 70.5793810005026\n",
            "At epoch 936, test loss: 70.57938123892114\n",
            "At epoch 937, test loss: 70.57938123892114\n",
            "At epoch 938, test loss: 70.57938123892114\n",
            "At epoch 939, test loss: 70.57938123892114\n",
            "At epoch 940, test loss: 70.57938123892114\n",
            "At epoch 941, test loss: 70.57938123892114\n",
            "At epoch 942, test loss: 70.57938123892114\n",
            "At epoch 943, test loss: 70.57938123892114\n",
            "At epoch 944, test loss: 70.57938123892114\n",
            "At epoch 945, test loss: 70.57938290784983\n",
            "At epoch 946, test loss: 70.57938290784983\n",
            "At epoch 947, test loss: 70.57938600728659\n",
            "At epoch 948, test loss: 70.57938612649588\n",
            "At epoch 949, test loss: 70.57940365010865\n",
            "At epoch 950, test loss: 70.57940365010865\n",
            "At epoch 951, test loss: 70.57940424615492\n",
            "At epoch 952, test loss: 70.57940424615492\n",
            "At epoch 953, test loss: 70.57942153135275\n",
            "At epoch 954, test loss: 70.57942320028144\n",
            "At epoch 955, test loss: 70.57942320028144\n",
            "At epoch 956, test loss: 70.57943082964687\n",
            "At epoch 957, test loss: 70.57946802225399\n",
            "At epoch 958, test loss: 70.57946802225399\n",
            "At epoch 959, test loss: 71.10612959709454\n",
            "At epoch 960, test loss: 71.10612959709454\n",
            "At epoch 961, test loss: 71.10612995472235\n",
            "At epoch 962, test loss: 71.10612995472235\n",
            "At epoch 963, test loss: 71.1061742995958\n",
            "At epoch 964, test loss: 71.66671813475892\n",
            "At epoch 965, test loss: 71.66681719277454\n",
            "At epoch 966, test loss: 71.66681719277454\n",
            "At epoch 967, test loss: 71.6668181464484\n",
            "At epoch 968, test loss: 71.6668181464484\n",
            "At epoch 969, test loss: 71.66719906678196\n",
            "At epoch 970, test loss: 71.66719978203747\n",
            "At epoch 971, test loss: 71.66719978203747\n",
            "At epoch 972, test loss: 71.66719978203747\n",
            "At epoch 973, test loss: 71.67284518521029\n",
            "At epoch 974, test loss: 71.67284518521029\n",
            "At epoch 975, test loss: 71.67284542362884\n",
            "At epoch 976, test loss: 71.67284542362884\n",
            "At epoch 977, test loss: 71.67284542362884\n",
            "At epoch 978, test loss: 71.67284542362884\n",
            "At epoch 979, test loss: 71.67284542362884\n",
            "At epoch 980, test loss: 71.67842669546351\n",
            "At epoch 981, test loss: 71.67842693388206\n",
            "At epoch 982, test loss: 71.67842693388206\n",
            "At epoch 983, test loss: 71.67842693388206\n",
            "At epoch 984, test loss: 71.67842693388206\n",
            "At epoch 985, test loss: 71.68218603864057\n",
            "At epoch 986, test loss: 71.68218842282363\n",
            "At epoch 987, test loss: 71.68218842282363\n",
            "At epoch 988, test loss: 71.68218842282363\n",
            "At epoch 989, test loss: 71.68218842282363\n",
            "At epoch 990, test loss: 71.85062210395404\n",
            "At epoch 991, test loss: 71.85062210395404\n",
            "At epoch 992, test loss: 71.85062210395404\n",
            "At epoch 993, test loss: 71.85062210395404\n",
            "At epoch 994, test loss: 71.85062210395404\n",
            "At epoch 995, test loss: 71.85072008918792\n",
            "At epoch 996, test loss: 71.85075084471168\n",
            "At epoch 997, test loss: 71.85075084471168\n",
            "At epoch 998, test loss: 74.35163013243141\n",
            "At epoch 999, test loss: 74.3519237018088\n",
            "At epoch 1000, test loss: 74.35193645712184\n",
            "Test Loss: 0.074352\n",
            "\n",
            "Test Accuracy of 0: 98% (109/111)\n",
            "Test Accuracy of 1: 100% (106/106)\n",
            "Test Accuracy of 2: 98% (109/111)\n",
            "Test Accuracy of 3: 100% (107/107)\n",
            "Test Accuracy of 4: 99% (109/110)\n",
            "Test Accuracy of 5: 98% (78/79)\n",
            "Test Accuracy of 6: 96% (74/77)\n",
            "Test Accuracy of 7: 100% (87/87)\n",
            "Test Accuracy of 8: 98% (93/94)\n",
            "Test Accuracy of 9: 95% (113/118)\n",
            "\n",
            "Test Accuracy (Overall): 98% (985/1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above operation takes 1 hr 37 mins for a test set of 1000 entries."
      ],
      "metadata": {
        "id": "X_1DPTyUJ782"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict(context, model, image, kernel_shape, stride):\n",
        "    # Encoding and encryption\n",
        "    x_enc, windows_nb = ts.im2col_encoding(\n",
        "            context, image.view(28, 28).tolist(), kernel_shape[0],\n",
        "            kernel_shape[1], stride\n",
        "        )\n",
        "\n",
        "    # Encrypted evaluation\n",
        "    enc_output = model(x_enc, windows_nb)\n",
        "    print(f'Encrypted image: {enc_output}')\n",
        "    # Decryption of result\n",
        "    output = enc_output.decrypt()\n",
        "    output = torch.tensor(output).view(1, -1)\n",
        "\n",
        "    # Obtain predicted label\n",
        "    _, pred = torch.max(output, 1)\n",
        "    return pred.item()\n",
        "\n",
        "random_image, random_label = test_data[521]\n",
        "# Convert the image to a NumPy array\n",
        "random_image_array = random_image.numpy().squeeze()\n",
        "# Display the original image using matplotlib\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(random_image_array, cmap='gray')\n",
        "plt.title(f\"Label: {random_label}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f'Predicted output: {predict(context, enc_model, random_image, kernel_shape, stride )}')"
      ],
      "metadata": {
        "id": "3dDMmsePi1OZ",
        "outputId": "de7185e3-9550-44f5-d7c6-4816c7913bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encrypted image: <tenseal.tensors.ckksvector.CKKSVector object at 0x7f5dbf440cd0>\n",
            "Predicted output: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPnxsr0uZyna"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}