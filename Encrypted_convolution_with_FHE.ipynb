{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPCSotynhT2PD+GGGKvmOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siya-9/Homomorphic-Encryption/blob/main/Encrypted_convolution_with_FHE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(73)\n",
        "\n",
        "train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw7R_q4MacY4",
        "outputId": "881f67ea-8a33-46de-fd3c-e88563ac253c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 89980784.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 24935301.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 29341799.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3599873.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(torch.nn.Module):\n",
        "    def __init__(self, hidden=64, output=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=7, padding=0, stride=3)\n",
        "        self.fc1 = torch.nn.Linear(256, hidden)\n",
        "        self.fc2 = torch.nn.Linear(hidden, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        # the model uses the square activation function\n",
        "        x = x * x\n",
        "        # flattening while keeping the batch axis\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.fc1(x)\n",
        "        x = x * x\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "ir_Ay0Zdac_8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, n_epochs=10):\n",
        "    # model in training mode\n",
        "    model.train()\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "\n",
        "        train_loss = 0.0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # calculate average losses\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
        "\n",
        "    # model in evaluation mode\n",
        "    model.eval()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "dJ2nzoXCcHFr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model = train(model, train_loader, criterion, optimizer, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7Hu-ZN6cJJy",
        "outputId": "32c9a661-cadd-4ef6-cf36-876cbd28609f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.397561\n",
            "Epoch: 2 \tTraining Loss: 0.130699\n",
            "Epoch: 3 \tTraining Loss: 0.088399\n",
            "Epoch: 4 \tTraining Loss: 0.071318\n",
            "Epoch: 5 \tTraining Loss: 0.058989\n",
            "Epoch: 6 \tTraining Loss: 0.050542\n",
            "Epoch: 7 \tTraining Loss: 0.044438\n",
            "Epoch: 8 \tTraining Loss: 0.038261\n",
            "Epoch: 9 \tTraining Loss: 0.034641\n",
            "Epoch: 10 \tTraining Loss: 0.030696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, criterion):\n",
        "    # initialize lists to monitor test loss and accuracy\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    # model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "    # calculate and print avg test loss\n",
        "    test_loss = test_loss/len(test_loader)\n",
        "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
        "\n",
        "    for label in range(10):\n",
        "        print(\n",
        "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
        "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% '\n",
        "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
        "    )\n",
        "\n",
        "test(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcd9qbNGdXK4",
        "outputId": "65bb94b2-6068-466a-b646-67cd903d4400"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.088565\n",
            "\n",
            "Test Accuracy of 0: 98% (966/980)\n",
            "Test Accuracy of 1: 99% (1130/1135)\n",
            "Test Accuracy of 2: 98% (1015/1032)\n",
            "Test Accuracy of 3: 99% (1002/1010)\n",
            "Test Accuracy of 4: 98% (967/982)\n",
            "Test Accuracy of 5: 98% (878/892)\n",
            "Test Accuracy of 6: 98% (944/958)\n",
            "Test Accuracy of 7: 98% (1011/1028)\n",
            "Test Accuracy of 8: 96% (941/974)\n",
            "Test Accuracy of 9: 96% (969/1009)\n",
            "\n",
            "Test Accuracy (Overall): 98% (9823/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenseal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J5VEl8fdrlm",
        "outputId": "632cd133-13ec-4afc-f1ed-d3c31757848f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tenseal\n",
            "  Downloading tenseal-0.3.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenseal\n",
            "Successfully installed tenseal-0.3.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It's a PyTorch-like model using operations implemented in TenSEAL.\n",
        "    - .mm() method is doing the vector-matrix multiplication explained above.\n",
        "    - you can use + operator to add a plain vector as a bias.\n",
        "    - .conv2d_im2col() method is doing a single convolution operation.\n",
        "    - .square_() just square the encrypted vector inplace.\n",
        "\"\"\"\n",
        "\n",
        "import tenseal as ts\n",
        "\n",
        "\n",
        "class EncConvNet:\n",
        "    def __init__(self, torch_nn):\n",
        "        self.conv1_weight = torch_nn.conv1.weight.data.view(\n",
        "            torch_nn.conv1.out_channels, torch_nn.conv1.kernel_size[0],\n",
        "            torch_nn.conv1.kernel_size[1]\n",
        "        ).tolist()\n",
        "        self.conv1_bias = torch_nn.conv1.bias.data.tolist()\n",
        "\n",
        "        self.fc1_weight = torch_nn.fc1.weight.T.data.tolist()\n",
        "        self.fc1_bias = torch_nn.fc1.bias.data.tolist()\n",
        "\n",
        "        self.fc2_weight = torch_nn.fc2.weight.T.data.tolist()\n",
        "        self.fc2_bias = torch_nn.fc2.bias.data.tolist()\n",
        "\n",
        "\n",
        "    def forward(self, enc_x, windows_nb):\n",
        "        # conv layer\n",
        "        enc_channels = []\n",
        "        for kernel, bias in zip(self.conv1_weight, self.conv1_bias):\n",
        "            y = enc_x.conv2d_im2col(kernel, windows_nb) + bias\n",
        "            enc_channels.append(y)\n",
        "        # pack all channels into a single flattened vector\n",
        "        enc_x = ts.CKKSVector.pack_vectors(enc_channels)\n",
        "        # square activation\n",
        "        enc_x.square_()\n",
        "        # fc1 layer\n",
        "        enc_x = enc_x.mm(self.fc1_weight) + self.fc1_bias\n",
        "        # square activation\n",
        "        enc_x.square_()\n",
        "        # fc2 layer\n",
        "        enc_x = enc_x.mm(self.fc2_weight) + self.fc2_bias\n",
        "        return enc_x\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n"
      ],
      "metadata": {
        "id": "5DVCL-IbdlgB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enc_test(context, model, test_loader, criterion, kernel_shape, stride):\n",
        "    # initialize lists to monitor test loss and accuracy\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    epoch = 0\n",
        "    for data, target in test_loader:\n",
        "        epoch += 1\n",
        "        # Encoding and encryption\n",
        "        x_enc, windows_nb = ts.im2col_encoding(\n",
        "            context, data.view(28, 28).tolist(), kernel_shape[0],\n",
        "            kernel_shape[1], stride\n",
        "        )\n",
        "        # Encrypted evaluation\n",
        "        enc_output = enc_model(x_enc, windows_nb)\n",
        "        # Decryption of result\n",
        "        output = enc_output.decrypt()\n",
        "        output = torch.tensor(output).view(1, -1)\n",
        "\n",
        "        # compute loss\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        print(f'At epoch {epoch}, test loss: {test_loss}')\n",
        "\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        label = target.data[0]\n",
        "        class_correct[label] += correct.item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "\n",
        "    # calculate and print avg test loss\n",
        "    test_loss = test_loss / sum(class_total)\n",
        "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
        "\n",
        "    for label in range(10):\n",
        "        print(\n",
        "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
        "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% '\n",
        "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "AQoC1x3HdoLy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test_samples = int(len(test_data)  * 0.1)\n",
        "test_indices = np.random.choice(len(test_data), size=num_test_samples, replace=False)\n",
        "test_data = torch.utils.data.Subset(test_data, test_indices)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "# Load one element at a time\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
        "# required for encoding\n",
        "kernel_shape = model.conv1.kernel_size\n",
        "stride = model.conv1.stride[0]"
      ],
      "metadata": {
        "id": "cR_SYedwis7C"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Encryption Parameters\n",
        "\n",
        "# controls precision of the fractional part\n",
        "bits_scale = 26\n",
        "\n",
        "# Create TenSEAL context\n",
        "context = ts.context(\n",
        "    ts.SCHEME_TYPE.CKKS,\n",
        "    poly_modulus_degree=8192,\n",
        "    coeff_mod_bit_sizes=[31, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, 31]\n",
        ")\n",
        "\n",
        "# set the scale\n",
        "context.global_scale = pow(2, bits_scale)\n",
        "\n",
        "# galois keys are required to do ciphertext rotations\n",
        "context.generate_galois_keys()"
      ],
      "metadata": {
        "id": "W9c8xTMld04G"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_model = EncConvNet(model)"
      ],
      "metadata": {
        "id": "u_PFNKN2d550"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_test(context, enc_model, test_loader, criterion, kernel_shape, stride)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldFUhGVohr1y",
        "outputId": "01769df5-1636-48ca-a9fd-ee528476a3a6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At epoch 1, test loss: 0.0\n",
            "At epoch 2, test loss: 0.00010632903286023065\n",
            "At epoch 3, test loss: 0.0007272166476468556\n",
            "At epoch 4, test loss: 0.0007288855763363244\n",
            "At epoch 5, test loss: 0.0008136397868838685\n",
            "At epoch 6, test loss: 0.0008136397868838685\n",
            "At epoch 7, test loss: 0.0008136397868838685\n",
            "At epoch 8, test loss: 0.0008136397868838685\n",
            "At epoch 9, test loss: 0.8933310276538577\n",
            "At epoch 10, test loss: 0.8933310276538577\n",
            "At epoch 11, test loss: 0.8933310276538577\n",
            "At epoch 12, test loss: 0.8933310276538577\n",
            "At epoch 13, test loss: 0.8933803790810089\n",
            "At epoch 14, test loss: 0.8933862203189165\n",
            "At epoch 15, test loss: 0.8936694214858107\n",
            "At epoch 16, test loss: 0.8936698983228553\n",
            "At epoch 17, test loss: 0.8936701367414059\n",
            "At epoch 18, test loss: 1.1133678505933347\n",
            "At epoch 19, test loss: 1.1133678505933347\n",
            "At epoch 20, test loss: 1.1133680890118853\n",
            "At epoch 21, test loss: 1.1133680890118853\n",
            "At epoch 22, test loss: 1.1133680890118853\n",
            "At epoch 23, test loss: 1.1133720229107098\n",
            "At epoch 24, test loss: 1.1133720229107098\n",
            "At epoch 25, test loss: 1.1133720229107098\n",
            "At epoch 26, test loss: 1.1134013479666578\n",
            "At epoch 27, test loss: 1.113742466774795\n",
            "At epoch 28, test loss: 1.113742466774795\n",
            "At epoch 29, test loss: 1.113742466774795\n",
            "At epoch 30, test loss: 1.113742466774795\n",
            "At epoch 31, test loss: 1.1137667851733681\n",
            "At epoch 32, test loss: 1.1137667851733681\n",
            "At epoch 33, test loss: 10.06374942830081\n",
            "At epoch 34, test loss: 10.063749905137854\n",
            "At epoch 35, test loss: 10.063753004574608\n",
            "At epoch 36, test loss: 10.06375312378389\n",
            "At epoch 37, test loss: 10.06375312378389\n",
            "At epoch 38, test loss: 10.06375312378389\n",
            "At epoch 39, test loss: 10.063753481411702\n",
            "At epoch 40, test loss: 10.063753481411702\n",
            "At epoch 41, test loss: 10.063758607397965\n",
            "At epoch 42, test loss: 10.063758607397965\n",
            "At epoch 43, test loss: 10.063772912410847\n",
            "At epoch 44, test loss: 10.063772912410847\n",
            "At epoch 45, test loss: 10.063772912410847\n",
            "At epoch 46, test loss: 10.063772912410847\n",
            "At epoch 47, test loss: 10.063772912410847\n",
            "At epoch 48, test loss: 10.063777084727342\n",
            "At epoch 49, test loss: 10.063777084727342\n",
            "At epoch 50, test loss: 12.388974542994553\n",
            "At epoch 51, test loss: 12.388974542994553\n",
            "At epoch 52, test loss: 12.388974542994553\n",
            "At epoch 53, test loss: 12.388974781413104\n",
            "At epoch 54, test loss: 12.388975139040916\n",
            "At epoch 55, test loss: 12.38901459653723\n",
            "At epoch 56, test loss: 12.38901459653723\n",
            "At epoch 57, test loss: 12.389564596126796\n",
            "At epoch 58, test loss: 12.389616212417671\n",
            "At epoch 59, test loss: 12.389616212417671\n",
            "At epoch 60, test loss: 12.389630755845495\n",
            "At epoch 61, test loss: 12.3900588639656\n",
            "At epoch 62, test loss: 12.390060532894289\n",
            "At epoch 63, test loss: 18.483309853878175\n",
            "At epoch 64, test loss: 18.492197821904277\n",
            "At epoch 65, test loss: 18.49219794111356\n",
            "At epoch 66, test loss: 18.49219794111356\n",
            "At epoch 67, test loss: 18.49219794111356\n",
            "At epoch 68, test loss: 18.49219794111356\n",
            "At epoch 69, test loss: 18.49219794111356\n",
            "At epoch 70, test loss: 18.49219794111356\n",
            "At epoch 71, test loss: 18.49219794111356\n",
            "At epoch 72, test loss: 18.492416785416722\n",
            "At epoch 73, test loss: 18.492416785416722\n",
            "At epoch 74, test loss: 18.492416785416722\n",
            "At epoch 75, test loss: 18.492416785416722\n",
            "At epoch 76, test loss: 18.492420361689042\n",
            "At epoch 77, test loss: 18.492420361689042\n",
            "At epoch 78, test loss: 18.492420361689042\n",
            "At epoch 79, test loss: 18.492456362246926\n",
            "At epoch 80, test loss: 18.492456362246926\n",
            "At epoch 81, test loss: 18.49246768706486\n",
            "At epoch 82, test loss: 18.49246768706486\n",
            "At epoch 83, test loss: 18.49246768706486\n",
            "At epoch 84, test loss: 18.492602146106734\n",
            "At epoch 85, test loss: 18.492602146106734\n",
            "At epoch 86, test loss: 18.49263766984565\n",
            "At epoch 87, test loss: 18.49263766984565\n",
            "At epoch 88, test loss: 18.49263766984565\n",
            "At epoch 89, test loss: 18.49263766984565\n",
            "At epoch 90, test loss: 18.52692745084984\n",
            "At epoch 91, test loss: 18.52692745084984\n",
            "At epoch 92, test loss: 18.532028378165492\n",
            "At epoch 93, test loss: 18.540452600634822\n",
            "At epoch 94, test loss: 18.540452600634822\n",
            "At epoch 95, test loss: 18.540452600634822\n",
            "At epoch 96, test loss: 18.540452600634822\n",
            "At epoch 97, test loss: 18.540864383921658\n",
            "At epoch 98, test loss: 18.54086533759552\n",
            "At epoch 99, test loss: 18.54086533759552\n",
            "At epoch 100, test loss: 18.54086533759552\n",
            "At epoch 101, test loss: 18.54086533759552\n",
            "At epoch 102, test loss: 18.540868198614376\n",
            "At epoch 103, test loss: 18.54086867545142\n",
            "At epoch 104, test loss: 18.540868794660703\n",
            "At epoch 105, test loss: 18.540868794660703\n",
            "At epoch 106, test loss: 18.540868794660703\n",
            "At epoch 107, test loss: 18.540868794660703\n",
            "At epoch 108, test loss: 18.540868794660703\n",
            "At epoch 109, test loss: 18.541176903205844\n",
            "At epoch 110, test loss: 18.541409811032857\n",
            "At epoch 111, test loss: 18.541409811032857\n",
            "At epoch 112, test loss: 18.54148359886188\n",
            "At epoch 113, test loss: 18.56249661846835\n",
            "At epoch 114, test loss: 18.56249661846835\n",
            "At epoch 115, test loss: 18.56249661846835\n",
            "At epoch 116, test loss: 18.56249661846835\n",
            "At epoch 117, test loss: 18.56249661846835\n",
            "At epoch 118, test loss: 18.562628693644577\n",
            "At epoch 119, test loss: 18.56263465409132\n",
            "At epoch 120, test loss: 18.56263465409132\n",
            "At epoch 121, test loss: 18.56263501171913\n",
            "At epoch 122, test loss: 18.56267542285338\n",
            "At epoch 123, test loss: 18.56267542285338\n",
            "At epoch 124, test loss: 18.56267542285338\n",
            "At epoch 125, test loss: 18.5626789991257\n",
            "At epoch 126, test loss: 18.5626789991257\n",
            "At epoch 127, test loss: 18.56320564626737\n",
            "At epoch 128, test loss: 18.56320564626737\n",
            "At epoch 129, test loss: 18.56320564626737\n",
            "At epoch 130, test loss: 18.56320731519606\n",
            "At epoch 131, test loss: 18.56320898412475\n",
            "At epoch 132, test loss: 18.56320934175256\n",
            "At epoch 133, test loss: 18.56321005700807\n",
            "At epoch 134, test loss: 18.606097595679564\n",
            "At epoch 135, test loss: 18.606097595679564\n",
            "At epoch 136, test loss: 18.606097595679564\n",
            "At epoch 137, test loss: 18.606097595679564\n",
            "At epoch 138, test loss: 18.606097714888847\n",
            "At epoch 139, test loss: 18.606253032529594\n",
            "At epoch 140, test loss: 18.606253032529594\n",
            "At epoch 141, test loss: 18.606253032529594\n",
            "At epoch 142, test loss: 18.606253032529594\n",
            "At epoch 143, test loss: 18.653417744630815\n",
            "At epoch 144, test loss: 18.653417744630815\n",
            "At epoch 145, test loss: 18.65438489856092\n",
            "At epoch 146, test loss: 18.65438489856092\n",
            "At epoch 147, test loss: 18.65438489856092\n",
            "At epoch 148, test loss: 18.65438489856092\n",
            "At epoch 149, test loss: 18.65438489856092\n",
            "At epoch 150, test loss: 18.65438489856092\n",
            "At epoch 151, test loss: 18.654385494607197\n",
            "At epoch 152, test loss: 18.655456612309138\n",
            "At epoch 153, test loss: 18.655456612309138\n",
            "At epoch 154, test loss: 18.655456612309138\n",
            "At epoch 155, test loss: 18.655456612309138\n",
            "At epoch 156, test loss: 18.655469725244586\n",
            "At epoch 157, test loss: 18.655469725244586\n",
            "At epoch 158, test loss: 18.655469725244586\n",
            "At epoch 159, test loss: 18.65546984445387\n",
            "At epoch 160, test loss: 18.65546984445387\n",
            "At epoch 161, test loss: 18.655485460748928\n",
            "At epoch 162, test loss: 18.655651982258455\n",
            "At epoch 163, test loss: 18.655655796948444\n",
            "At epoch 164, test loss: 18.655655796948444\n",
            "At epoch 165, test loss: 18.655655796948444\n",
            "At epoch 166, test loss: 18.655655796948444\n",
            "At epoch 167, test loss: 18.65628835973944\n",
            "At epoch 168, test loss: 18.656298850102353\n",
            "At epoch 169, test loss: 18.656336400322793\n",
            "At epoch 170, test loss: 18.656336400322793\n",
            "At epoch 171, test loss: 18.656336400322793\n",
            "At epoch 172, test loss: 18.656336400322793\n",
            "At epoch 173, test loss: 18.656336400322793\n",
            "At epoch 174, test loss: 18.656424611304423\n",
            "At epoch 175, test loss: 18.656425564978285\n",
            "At epoch 176, test loss: 18.65653380115605\n",
            "At epoch 177, test loss: 18.656534158783863\n",
            "At epoch 178, test loss: 18.656534158783863\n",
            "At epoch 179, test loss: 18.656534158783863\n",
            "At epoch 180, test loss: 18.656534277993146\n",
            "At epoch 181, test loss: 18.656534277993146\n",
            "At epoch 182, test loss: 18.656534277993146\n",
            "At epoch 183, test loss: 18.656534277993146\n",
            "At epoch 184, test loss: 18.656534277993146\n",
            "At epoch 185, test loss: 18.656537854265466\n",
            "At epoch 186, test loss: 18.656537854265466\n",
            "At epoch 187, test loss: 18.656538211893277\n",
            "At epoch 188, test loss: 18.656538211893277\n",
            "At epoch 189, test loss: 18.656538450311828\n",
            "At epoch 190, test loss: 18.658833573492203\n",
            "At epoch 191, test loss: 18.658845136725986\n",
            "At epoch 192, test loss: 18.658845136725986\n",
            "At epoch 193, test loss: 18.658927268550443\n",
            "At epoch 194, test loss: 18.658927268550443\n",
            "At epoch 195, test loss: 18.65892929510632\n",
            "At epoch 196, test loss: 18.65892929510632\n",
            "At epoch 197, test loss: 18.65892929510632\n",
            "At epoch 198, test loss: 18.65892929510632\n",
            "At epoch 199, test loss: 18.65892929510632\n",
            "At epoch 200, test loss: 18.658930606407594\n",
            "At epoch 201, test loss: 18.658930606407594\n",
            "At epoch 202, test loss: 18.658931321663104\n",
            "At epoch 203, test loss: 18.661150203983553\n",
            "At epoch 204, test loss: 18.661153661047038\n",
            "At epoch 205, test loss: 18.6611951450176\n",
            "At epoch 206, test loss: 18.661196456318876\n",
            "At epoch 207, test loss: 18.661284905697258\n",
            "At epoch 208, test loss: 18.66128645541677\n",
            "At epoch 209, test loss: 18.66136322325248\n",
            "At epoch 210, test loss: 18.66137323678278\n",
            "At epoch 211, test loss: 18.66137323678278\n",
            "At epoch 212, test loss: 19.334105709202518\n",
            "At epoch 213, test loss: 19.334177589821152\n",
            "At epoch 214, test loss: 19.334177589821152\n",
            "At epoch 215, test loss: 19.33418009321305\n",
            "At epoch 216, test loss: 19.33418009321305\n",
            "At epoch 217, test loss: 19.33419785523894\n",
            "At epoch 218, test loss: 19.33419785523894\n",
            "At epoch 219, test loss: 19.33419785523894\n",
            "At epoch 220, test loss: 19.33419785523894\n",
            "At epoch 221, test loss: 19.3342070343124\n",
            "At epoch 222, test loss: 19.514859801732214\n",
            "At epoch 223, test loss: 19.514859801732214\n",
            "At epoch 224, test loss: 19.514859801732214\n",
            "At epoch 225, test loss: 19.514859801732214\n",
            "At epoch 226, test loss: 19.514859801732214\n",
            "At epoch 227, test loss: 19.514859801732214\n",
            "At epoch 228, test loss: 19.514862781959906\n",
            "At epoch 229, test loss: 19.514862781959906\n",
            "At epoch 230, test loss: 19.514862781959906\n",
            "At epoch 231, test loss: 19.514862781959906\n",
            "At epoch 232, test loss: 19.548287334489956\n",
            "At epoch 233, test loss: 19.548287811327\n",
            "At epoch 234, test loss: 19.548288407373278\n",
            "At epoch 235, test loss: 19.548288407373278\n",
            "At epoch 236, test loss: 19.548288407373278\n",
            "At epoch 237, test loss: 19.548300924270464\n",
            "At epoch 238, test loss: 19.548300924270464\n",
            "At epoch 239, test loss: 20.34489280220778\n",
            "At epoch 240, test loss: 20.34489280220778\n",
            "At epoch 241, test loss: 20.344893636672467\n",
            "At epoch 242, test loss: 20.34489816661501\n",
            "At epoch 243, test loss: 20.422348434585572\n",
            "At epoch 244, test loss: 20.432350923787297\n",
            "At epoch 245, test loss: 20.432350923787297\n",
            "At epoch 246, test loss: 20.432350923787297\n",
            "At epoch 247, test loss: 20.432350923787297\n",
            "At epoch 248, test loss: 20.432385732292303\n",
            "At epoch 249, test loss: 20.432385732292303\n",
            "At epoch 250, test loss: 20.432400037305186\n",
            "At epoch 251, test loss: 20.432400037305186\n",
            "At epoch 252, test loss: 20.432400037305186\n",
            "At epoch 253, test loss: 20.432400037305186\n",
            "At epoch 254, test loss: 20.547986042456458\n",
            "At epoch 255, test loss: 20.54798914189321\n",
            "At epoch 256, test loss: 20.54798914189321\n",
            "At epoch 257, test loss: 20.54798914189321\n",
            "At epoch 258, test loss: 20.556564661440078\n",
            "At epoch 259, test loss: 20.556564661440078\n",
            "At epoch 260, test loss: 20.934912296709243\n",
            "At epoch 261, test loss: 22.280810924944106\n",
            "At epoch 262, test loss: 22.280810924944106\n",
            "At epoch 263, test loss: 22.280810924944106\n",
            "At epoch 264, test loss: 22.55244288104457\n",
            "At epoch 265, test loss: 22.55244586127226\n",
            "At epoch 266, test loss: 22.552484245924987\n",
            "At epoch 267, test loss: 22.55248436513427\n",
            "At epoch 268, test loss: 22.600418535946638\n",
            "At epoch 269, test loss: 22.60162885040115\n",
            "At epoch 270, test loss: 22.60162956565666\n",
            "At epoch 271, test loss: 22.60162956565666\n",
            "At epoch 272, test loss: 22.61504063960212\n",
            "At epoch 273, test loss: 22.61504564637955\n",
            "At epoch 274, test loss: 22.61504564637955\n",
            "At epoch 275, test loss: 22.61504946106954\n",
            "At epoch 276, test loss: 22.61504946106954\n",
            "At epoch 277, test loss: 22.61504969948809\n",
            "At epoch 278, test loss: 22.615050891580303\n",
            "At epoch 279, test loss: 22.615051010789585\n",
            "At epoch 280, test loss: 22.615051010789585\n",
            "At epoch 281, test loss: 22.615051010789585\n",
            "At epoch 282, test loss: 22.615051010789585\n",
            "At epoch 283, test loss: 22.615051010789585\n",
            "At epoch 284, test loss: 22.615051010789585\n",
            "At epoch 285, test loss: 22.645468417721105\n",
            "At epoch 286, test loss: 22.645468417721105\n",
            "At epoch 287, test loss: 22.672748858275128\n",
            "At epoch 288, test loss: 22.67275338821767\n",
            "At epoch 289, test loss: 22.687111335030778\n",
            "At epoch 290, test loss: 22.687111335030778\n",
            "At epoch 291, test loss: 22.687112050286288\n",
            "At epoch 292, test loss: 22.68711216949557\n",
            "At epoch 293, test loss: 22.688030374624553\n",
            "At epoch 294, test loss: 22.688030374624553\n",
            "At epoch 295, test loss: 31.710469377615276\n",
            "At epoch 296, test loss: 31.710469377615276\n",
            "At epoch 297, test loss: 31.710469377615276\n",
            "At epoch 298, test loss: 31.71064889069637\n",
            "At epoch 299, test loss: 31.710649367533414\n",
            "At epoch 300, test loss: 31.710649486742696\n",
            "At epoch 301, test loss: 31.710649486742696\n",
            "At epoch 302, test loss: 31.710649486742696\n",
            "At epoch 303, test loss: 31.71064960595198\n",
            "At epoch 304, test loss: 31.710767616185542\n",
            "At epoch 305, test loss: 31.711114931723053\n",
            "At epoch 306, test loss: 31.711114931723053\n",
            "At epoch 307, test loss: 31.711114931723053\n",
            "At epoch 308, test loss: 31.928505707403595\n",
            "At epoch 309, test loss: 31.928505707403595\n",
            "At epoch 310, test loss: 31.928513813602542\n",
            "At epoch 311, test loss: 31.928513813602542\n",
            "At epoch 312, test loss: 31.928513813602542\n",
            "At epoch 313, test loss: 31.928567337140187\n",
            "At epoch 314, test loss: 31.928567337140187\n",
            "At epoch 315, test loss: 31.92856745634947\n",
            "At epoch 316, test loss: 31.92856745634947\n",
            "At epoch 317, test loss: 31.92856745634947\n",
            "At epoch 318, test loss: 31.92856745634947\n",
            "At epoch 319, test loss: 31.92856745634947\n",
            "At epoch 320, test loss: 31.92856745634947\n",
            "At epoch 321, test loss: 31.928567933186514\n",
            "At epoch 322, test loss: 31.928567933186514\n",
            "At epoch 323, test loss: 31.937084354728086\n",
            "At epoch 324, test loss: 31.937092937760553\n",
            "At epoch 325, test loss: 31.937092937760553\n",
            "At epoch 326, test loss: 31.937187585454467\n",
            "At epoch 327, test loss: 31.937188419919153\n",
            "At epoch 328, test loss: 31.937188419919153\n",
            "At epoch 329, test loss: 31.937696480015916\n",
            "At epoch 330, test loss: 31.94490110390882\n",
            "At epoch 331, test loss: 31.94490789881513\n",
            "At epoch 332, test loss: 31.94490789881513\n",
            "At epoch 333, test loss: 31.944909448534645\n",
            "At epoch 334, test loss: 31.959871331960358\n",
            "At epoch 335, test loss: 31.959871331960358\n",
            "At epoch 336, test loss: 31.959871331960358\n",
            "At epoch 337, test loss: 31.959878842117405\n",
            "At epoch 338, test loss: 31.959878961326687\n",
            "At epoch 339, test loss: 31.959878961326687\n",
            "At epoch 340, test loss: 31.9598899285213\n",
            "At epoch 341, test loss: 31.9598899285213\n",
            "At epoch 342, test loss: 31.9598899285213\n",
            "At epoch 343, test loss: 31.961085002463953\n",
            "At epoch 344, test loss: 31.961085002463953\n",
            "At epoch 345, test loss: 31.961085002463953\n",
            "At epoch 346, test loss: 31.961085002463953\n",
            "At epoch 347, test loss: 33.14007536653866\n",
            "At epoch 348, test loss: 33.14007536653866\n",
            "At epoch 349, test loss: 33.14007572416647\n",
            "At epoch 350, test loss: 33.14007572416647\n",
            "At epoch 351, test loss: 33.14010051939129\n",
            "At epoch 352, test loss: 33.140108864007\n",
            "At epoch 353, test loss: 33.140108864007\n",
            "At epoch 354, test loss: 33.140108864007\n",
            "At epoch 355, test loss: 33.140124361094585\n",
            "At epoch 356, test loss: 33.143410919065154\n",
            "At epoch 357, test loss: 33.143410919065154\n",
            "At epoch 358, test loss: 33.425877747888244\n",
            "At epoch 359, test loss: 33.42590683453107\n",
            "At epoch 360, test loss: 33.42590683453107\n",
            "At epoch 361, test loss: 33.42590683453107\n",
            "At epoch 362, test loss: 33.42590683453107\n",
            "At epoch 363, test loss: 33.42590719215888\n",
            "At epoch 364, test loss: 33.42590719215888\n",
            "At epoch 365, test loss: 33.42590719215888\n",
            "At epoch 366, test loss: 33.42605214014849\n",
            "At epoch 367, test loss: 33.42605500116735\n",
            "At epoch 368, test loss: 33.42605500116735\n",
            "At epoch 369, test loss: 33.42605500116735\n",
            "At epoch 370, test loss: 33.45338276967197\n",
            "At epoch 371, test loss: 33.45338276967197\n",
            "At epoch 372, test loss: 33.45338276967197\n",
            "At epoch 373, test loss: 33.45338276967197\n",
            "At epoch 374, test loss: 33.45338276967197\n",
            "At epoch 375, test loss: 33.506868006733946\n",
            "At epoch 376, test loss: 33.506868006733946\n",
            "At epoch 377, test loss: 33.506877066599024\n",
            "At epoch 378, test loss: 33.50701450546568\n",
            "At epoch 379, test loss: 33.50701450546568\n",
            "At epoch 380, test loss: 33.509718566102144\n",
            "At epoch 381, test loss: 33.510456080390775\n",
            "At epoch 382, test loss: 33.510491127307084\n",
            "At epoch 383, test loss: 33.51628196627882\n",
            "At epoch 384, test loss: 33.516294602383475\n",
            "At epoch 385, test loss: 33.516294602383475\n",
            "At epoch 386, test loss: 33.516294602383475\n",
            "At epoch 387, test loss: 33.51629472159276\n",
            "At epoch 388, test loss: 33.51629472159276\n",
            "At epoch 389, test loss: 33.51631808634066\n",
            "At epoch 390, test loss: 33.51632345074414\n",
            "At epoch 391, test loss: 33.51632356995342\n",
            "At epoch 392, test loss: 33.51632356995342\n",
            "At epoch 393, test loss: 33.51632356995342\n",
            "At epoch 394, test loss: 33.51632356995342\n",
            "At epoch 395, test loss: 33.51770294250546\n",
            "At epoch 396, test loss: 33.51770294250546\n",
            "At epoch 397, test loss: 33.51770294250546\n",
            "At epoch 398, test loss: 33.517868391306\n",
            "At epoch 399, test loss: 39.56232439640244\n",
            "At epoch 400, test loss: 39.56232439640244\n",
            "At epoch 401, test loss: 39.56232439640244\n",
            "At epoch 402, test loss: 39.56232439640244\n",
            "At epoch 403, test loss: 39.562325707703714\n",
            "At epoch 404, test loss: 39.562325707703714\n",
            "At epoch 405, test loss: 39.562325707703714\n",
            "At epoch 406, test loss: 39.562325707703714\n",
            "At epoch 407, test loss: 39.56236254269752\n",
            "At epoch 408, test loss: 44.68371855313453\n",
            "At epoch 409, test loss: 44.68378745373193\n",
            "At epoch 410, test loss: 44.68378745373193\n",
            "At epoch 411, test loss: 44.683863387168095\n",
            "At epoch 412, test loss: 44.68394587659132\n",
            "At epoch 413, test loss: 44.68464979807468\n",
            "At epoch 414, test loss: 44.68465611614702\n",
            "At epoch 415, test loss: 44.68465635456557\n",
            "At epoch 416, test loss: 44.68465647377485\n",
            "At epoch 417, test loss: 44.68469676570345\n",
            "At epoch 418, test loss: 44.68469676570345\n",
            "At epoch 419, test loss: 44.68469688491273\n",
            "At epoch 420, test loss: 44.68480881617102\n",
            "At epoch 421, test loss: 44.68480881617102\n",
            "At epoch 422, test loss: 44.68480881617102\n",
            "At epoch 423, test loss: 44.68480881617102\n",
            "At epoch 424, test loss: 44.68480881617102\n",
            "At epoch 425, test loss: 44.68480881617102\n",
            "At epoch 426, test loss: 44.68480881617102\n",
            "At epoch 427, test loss: 44.68480881617102\n",
            "At epoch 428, test loss: 44.68480881617102\n",
            "At epoch 429, test loss: 44.68480881617102\n",
            "At epoch 430, test loss: 44.68480881617102\n",
            "At epoch 431, test loss: 44.68510762921465\n",
            "At epoch 432, test loss: 44.685221586805305\n",
            "At epoch 433, test loss: 44.685221586805305\n",
            "At epoch 434, test loss: 44.68525162709516\n",
            "At epoch 435, test loss: 44.685252819187376\n",
            "At epoch 436, test loss: 44.685252819187376\n",
            "At epoch 437, test loss: 44.68525293839666\n",
            "At epoch 438, test loss: 44.68525293839666\n",
            "At epoch 439, test loss: 44.68525293839666\n",
            "At epoch 440, test loss: 44.68525293839666\n",
            "At epoch 441, test loss: 44.68525293839666\n",
            "At epoch 442, test loss: 44.68525293839666\n",
            "At epoch 443, test loss: 44.68525293839666\n",
            "At epoch 444, test loss: 44.68525305760594\n",
            "At epoch 445, test loss: 44.68535926744406\n",
            "At epoch 446, test loss: 44.68535926744406\n",
            "At epoch 447, test loss: 44.68535926744406\n",
            "At epoch 448, test loss: 44.68535926744406\n",
            "At epoch 449, test loss: 44.68535926744406\n",
            "At epoch 450, test loss: 44.6853819169526\n",
            "At epoch 451, test loss: 44.68538203616188\n",
            "At epoch 452, test loss: 44.68538203616188\n",
            "At epoch 453, test loss: 44.68574961006087\n",
            "At epoch 454, test loss: 44.68575044452555\n",
            "At epoch 455, test loss: 44.68575044452555\n",
            "At epoch 456, test loss: 44.68575044452555\n",
            "At epoch 457, test loss: 44.68581767630204\n",
            "At epoch 458, test loss: 44.68581767630204\n",
            "At epoch 459, test loss: 44.68581958364885\n",
            "At epoch 460, test loss: 44.685820894950126\n",
            "At epoch 461, test loss: 44.68582125257794\n",
            "At epoch 462, test loss: 44.68582125257794\n",
            "At epoch 463, test loss: 44.68582125257794\n",
            "At epoch 464, test loss: 45.50132788967434\n",
            "At epoch 465, test loss: 45.50190898530977\n",
            "At epoch 466, test loss: 45.50190898530977\n",
            "At epoch 467, test loss: 45.50190898530977\n",
            "At epoch 468, test loss: 45.501909819774454\n",
            "At epoch 469, test loss: 45.50190993898374\n",
            "At epoch 470, test loss: 45.50190993898374\n",
            "At epoch 471, test loss: 45.50190993898374\n",
            "At epoch 472, test loss: 45.50190993898374\n",
            "At epoch 473, test loss: 46.17255469911557\n",
            "At epoch 474, test loss: 46.17255469911557\n",
            "At epoch 475, test loss: 46.17255517595262\n",
            "At epoch 476, test loss: 46.17255517595262\n",
            "At epoch 477, test loss: 46.17255517595262\n",
            "At epoch 478, test loss: 46.17266925273801\n",
            "At epoch 479, test loss: 46.17266925273801\n",
            "At epoch 480, test loss: 46.172674378724274\n",
            "At epoch 481, test loss: 46.172674378724274\n",
            "At epoch 482, test loss: 46.172674617142825\n",
            "At epoch 483, test loss: 46.175831892638875\n",
            "At epoch 484, test loss: 46.1759133092658\n",
            "At epoch 485, test loss: 46.17591473977625\n",
            "At epoch 486, test loss: 46.17591473977625\n",
            "At epoch 487, test loss: 46.17591473977625\n",
            "At epoch 488, test loss: 46.17975649941325\n",
            "At epoch 489, test loss: 46.180242755065755\n",
            "At epoch 490, test loss: 46.180242755065755\n",
            "At epoch 491, test loss: 46.18025419909207\n",
            "At epoch 492, test loss: 46.18025491434758\n",
            "At epoch 493, test loss: 46.18025539118462\n",
            "At epoch 494, test loss: 46.18040391492905\n",
            "At epoch 495, test loss: 46.18040391492905\n",
            "At epoch 496, test loss: 51.5518347887999\n",
            "At epoch 497, test loss: 51.5518347887999\n",
            "At epoch 498, test loss: 51.55183550405541\n",
            "At epoch 499, test loss: 51.55183550405541\n",
            "At epoch 500, test loss: 51.55183550405541\n",
            "At epoch 501, test loss: 51.552591481988706\n",
            "At epoch 502, test loss: 51.846380004470625\n",
            "At epoch 503, test loss: 51.84638083893531\n",
            "At epoch 504, test loss: 51.940788434629376\n",
            "At epoch 505, test loss: 51.940788434629376\n",
            "At epoch 506, test loss: 51.940788434629376\n",
            "At epoch 507, test loss: 51.977126056139404\n",
            "At epoch 508, test loss: 51.97712617534869\n",
            "At epoch 509, test loss: 51.97712617534869\n",
            "At epoch 510, test loss: 66.2211571278023\n",
            "At epoch 511, test loss: 66.23742583933942\n",
            "At epoch 512, test loss: 66.23742583933942\n",
            "At epoch 513, test loss: 67.80538812819593\n",
            "At epoch 514, test loss: 67.96585137191884\n",
            "At epoch 515, test loss: 90.2552190477001\n",
            "At epoch 516, test loss: 90.25522023979231\n",
            "At epoch 517, test loss: 90.25913265468209\n",
            "At epoch 518, test loss: 90.25913289310064\n",
            "At epoch 519, test loss: 90.25913289310064\n",
            "At epoch 520, test loss: 90.25913301230992\n",
            "At epoch 521, test loss: 90.2627927083953\n",
            "At epoch 522, test loss: 90.2627927083953\n",
            "At epoch 523, test loss: 90.2627927083953\n",
            "At epoch 524, test loss: 90.2627927083953\n",
            "At epoch 525, test loss: 90.26581497983659\n",
            "At epoch 526, test loss: 90.26581497983659\n",
            "At epoch 527, test loss: 90.26582809277204\n",
            "At epoch 528, test loss: 90.2658309537909\n",
            "At epoch 529, test loss: 90.26583119220945\n",
            "At epoch 530, test loss: 90.26583119220945\n",
            "At epoch 531, test loss: 90.26583119220945\n",
            "At epoch 532, test loss: 90.26583119220945\n",
            "At epoch 533, test loss: 90.26583119220945\n",
            "At epoch 534, test loss: 90.26633520128584\n",
            "At epoch 535, test loss: 90.26633520128584\n",
            "At epoch 536, test loss: 90.26633520128584\n",
            "At epoch 537, test loss: 90.26633520128584\n",
            "At epoch 538, test loss: 90.26633520128584\n",
            "At epoch 539, test loss: 90.26633520128584\n",
            "At epoch 540, test loss: 90.27189963946498\n",
            "At epoch 541, test loss: 90.30929383153475\n",
            "At epoch 542, test loss: 90.30929383153475\n",
            "At epoch 543, test loss: 90.30929383153475\n",
            "At epoch 544, test loss: 90.30971002376086\n",
            "At epoch 545, test loss: 90.30971002376086\n",
            "At epoch 546, test loss: 90.30971038138867\n",
            "At epoch 547, test loss: 90.30971050059796\n",
            "At epoch 548, test loss: 90.30975830238074\n",
            "At epoch 549, test loss: 90.30975866000855\n",
            "At epoch 550, test loss: 90.30975866000855\n",
            "At epoch 551, test loss: 90.30975866000855\n",
            "At epoch 552, test loss: 90.30975866000855\n",
            "At epoch 553, test loss: 91.80350958984101\n",
            "At epoch 554, test loss: 91.80350958984101\n",
            "At epoch 555, test loss: 91.80350958984101\n",
            "At epoch 556, test loss: 91.80365930550371\n",
            "At epoch 557, test loss: 91.80365930550371\n",
            "At epoch 558, test loss: 91.80365930550371\n",
            "At epoch 559, test loss: 91.80366967665825\n",
            "At epoch 560, test loss: 91.80366967665825\n",
            "At epoch 561, test loss: 91.8036699150768\n",
            "At epoch 562, test loss: 91.8036699150768\n",
            "At epoch 563, test loss: 91.8036699150768\n",
            "At epoch 564, test loss: 91.87338025170237\n",
            "At epoch 565, test loss: 91.87338060933018\n",
            "At epoch 566, test loss: 91.87338060933018\n",
            "At epoch 567, test loss: 91.87338060933018\n",
            "At epoch 568, test loss: 91.87338442402017\n",
            "At epoch 569, test loss: 91.87338513927568\n",
            "At epoch 570, test loss: 91.87338513927568\n",
            "At epoch 571, test loss: 91.88725588889827\n",
            "At epoch 572, test loss: 91.88725588889827\n",
            "At epoch 573, test loss: 91.88725588889827\n",
            "At epoch 574, test loss: 91.88725588889827\n",
            "At epoch 575, test loss: 91.88725588889827\n",
            "At epoch 576, test loss: 91.88725588889827\n",
            "At epoch 577, test loss: 91.88842464878724\n",
            "At epoch 578, test loss: 91.88842476799653\n",
            "At epoch 579, test loss: 91.88842476799653\n",
            "At epoch 580, test loss: 91.88842476799653\n",
            "At epoch 581, test loss: 91.88842476799653\n",
            "At epoch 582, test loss: 91.88843180132005\n",
            "At epoch 583, test loss: 91.88843180132005\n",
            "At epoch 584, test loss: 91.88843180132005\n",
            "At epoch 585, test loss: 91.88843180132005\n",
            "At epoch 586, test loss: 91.88869581503002\n",
            "At epoch 587, test loss: 91.88869855683981\n",
            "At epoch 588, test loss: 91.88869855683981\n",
            "At epoch 589, test loss: 91.88874933870932\n",
            "At epoch 590, test loss: 91.88881359045001\n",
            "At epoch 591, test loss: 91.88881359045001\n",
            "At epoch 592, test loss: 91.89075422152484\n",
            "At epoch 593, test loss: 91.89075422152484\n",
            "At epoch 594, test loss: 91.89075422152484\n",
            "At epoch 595, test loss: 91.89075422152484\n",
            "At epoch 596, test loss: 91.89075422152484\n",
            "At epoch 597, test loss: 91.89075422152484\n",
            "At epoch 598, test loss: 103.30524301394428\n",
            "At epoch 599, test loss: 103.30524301394428\n",
            "At epoch 600, test loss: 103.30524301394428\n",
            "At epoch 601, test loss: 103.30570007708695\n",
            "At epoch 602, test loss: 103.30570007708695\n",
            "At epoch 603, test loss: 103.30570114996999\n",
            "At epoch 604, test loss: 103.30599257422045\n",
            "At epoch 605, test loss: 103.30599257422045\n",
            "At epoch 606, test loss: 103.33519597237364\n",
            "At epoch 607, test loss: 103.33519597237364\n",
            "At epoch 608, test loss: 103.3352067011524\n",
            "At epoch 609, test loss: 103.33520825087191\n",
            "At epoch 610, test loss: 103.33520825087191\n",
            "At epoch 611, test loss: 103.33520825087191\n",
            "At epoch 612, test loss: 103.53128445028426\n",
            "At epoch 613, test loss: 103.53128445028426\n",
            "At epoch 614, test loss: 103.53128445028426\n",
            "At epoch 615, test loss: 103.53128445028426\n",
            "At epoch 616, test loss: 103.53128445028426\n",
            "At epoch 617, test loss: 103.53128445028426\n",
            "At epoch 618, test loss: 103.53128445028426\n",
            "At epoch 619, test loss: 103.5747804033673\n",
            "At epoch 620, test loss: 103.5747804033673\n",
            "At epoch 621, test loss: 103.57478088020434\n",
            "At epoch 622, test loss: 103.57478088020434\n",
            "At epoch 623, test loss: 104.38532256960025\n",
            "At epoch 624, test loss: 104.38532256960025\n",
            "At epoch 625, test loss: 104.38532256960025\n",
            "At epoch 626, test loss: 104.38532256960025\n",
            "At epoch 627, test loss: 104.38532256960025\n",
            "At epoch 628, test loss: 104.3853300797573\n",
            "At epoch 629, test loss: 104.3853300797573\n",
            "At epoch 630, test loss: 104.3853300797573\n",
            "At epoch 631, test loss: 104.3853300797573\n",
            "At epoch 632, test loss: 104.3853300797573\n",
            "At epoch 633, test loss: 104.38549469423833\n",
            "At epoch 634, test loss: 104.38549493265688\n",
            "At epoch 635, test loss: 104.38549493265688\n",
            "At epoch 636, test loss: 104.38549493265688\n",
            "At epoch 637, test loss: 104.38549505186617\n",
            "At epoch 638, test loss: 104.38549505186617\n",
            "At epoch 639, test loss: 104.38557396530175\n",
            "At epoch 640, test loss: 104.39253593496048\n",
            "At epoch 641, test loss: 113.1757342915889\n",
            "At epoch 642, test loss: 113.48689989260399\n",
            "At epoch 643, test loss: 113.48689989260399\n",
            "At epoch 644, test loss: 113.48689989260399\n",
            "At epoch 645, test loss: 113.48689989260399\n",
            "At epoch 646, test loss: 113.4869002502318\n",
            "At epoch 647, test loss: 113.4869002502318\n",
            "At epoch 648, test loss: 113.48690036944109\n",
            "At epoch 649, test loss: 113.48753936545918\n",
            "At epoch 650, test loss: 113.48753936545918\n",
            "At epoch 651, test loss: 113.48782590352576\n",
            "At epoch 652, test loss: 113.48790290975822\n",
            "At epoch 653, test loss: 113.48919097067201\n",
            "At epoch 654, test loss: 113.48919097067201\n",
            "At epoch 655, test loss: 113.48919097067201\n",
            "At epoch 656, test loss: 113.49583645456815\n",
            "At epoch 657, test loss: 113.49588103784927\n",
            "At epoch 658, test loss: 113.49588103784927\n",
            "At epoch 659, test loss: 113.49588103784927\n",
            "At epoch 660, test loss: 113.49588389886813\n",
            "At epoch 661, test loss: 113.49588401807742\n",
            "At epoch 662, test loss: 113.49589021694138\n",
            "At epoch 663, test loss: 113.49589021694138\n",
            "At epoch 664, test loss: 113.49589021694138\n",
            "At epoch 665, test loss: 113.49589021694138\n",
            "At epoch 666, test loss: 113.49589021694138\n",
            "At epoch 667, test loss: 113.49589021694138\n",
            "At epoch 668, test loss: 113.49589021694138\n",
            "At epoch 669, test loss: 113.49589021694138\n",
            "At epoch 670, test loss: 118.6787648965068\n",
            "At epoch 671, test loss: 118.67913282788814\n",
            "At epoch 672, test loss: 118.6793219951318\n",
            "At epoch 673, test loss: 118.6793219951318\n",
            "At epoch 674, test loss: 118.67943297282491\n",
            "At epoch 675, test loss: 118.67943297282491\n",
            "At epoch 676, test loss: 118.67943297282491\n",
            "At epoch 677, test loss: 118.67943297282491\n",
            "At epoch 678, test loss: 118.67943297282491\n",
            "At epoch 679, test loss: 118.67943416491713\n",
            "At epoch 680, test loss: 118.67943416491713\n",
            "At epoch 681, test loss: 118.67943786039828\n",
            "At epoch 682, test loss: 118.69195426024376\n",
            "At epoch 683, test loss: 118.69200432689027\n",
            "At epoch 684, test loss: 119.07905856163339\n",
            "At epoch 685, test loss: 119.07905856163339\n",
            "At epoch 686, test loss: 119.07905856163339\n",
            "At epoch 687, test loss: 119.08011598481895\n",
            "At epoch 688, test loss: 119.08026498532777\n",
            "At epoch 689, test loss: 119.08026570058328\n",
            "At epoch 690, test loss: 120.62862080312235\n",
            "At epoch 691, test loss: 120.62862080312235\n",
            "At epoch 692, test loss: 120.62862092233163\n",
            "At epoch 693, test loss: 120.62862092233163\n",
            "At epoch 694, test loss: 120.62862092233163\n",
            "At epoch 695, test loss: 120.62862092233163\n",
            "At epoch 696, test loss: 120.62892628990713\n",
            "At epoch 697, test loss: 120.62892628990713\n",
            "At epoch 698, test loss: 120.63131608179438\n",
            "At epoch 699, test loss: 120.63131608179438\n",
            "At epoch 700, test loss: 120.63135267837689\n",
            "At epoch 701, test loss: 120.63135267837689\n",
            "At epoch 702, test loss: 120.63135267837689\n",
            "At epoch 703, test loss: 120.63135279758617\n",
            "At epoch 704, test loss: 120.63135279758617\n",
            "At epoch 705, test loss: 120.63135279758617\n",
            "At epoch 706, test loss: 120.63135279758617\n",
            "At epoch 707, test loss: 120.63135279758617\n",
            "At epoch 708, test loss: 120.63138128820074\n",
            "At epoch 709, test loss: 120.63138128820074\n",
            "At epoch 710, test loss: 120.63138128820074\n",
            "At epoch 711, test loss: 120.63322637524189\n",
            "At epoch 712, test loss: 120.63330409667915\n",
            "At epoch 713, test loss: 120.63330409667915\n",
            "At epoch 714, test loss: 120.63330409667915\n",
            "At epoch 715, test loss: 120.63333008396594\n",
            "At epoch 716, test loss: 120.63333008396594\n",
            "At epoch 717, test loss: 120.63333008396594\n",
            "At epoch 718, test loss: 120.633441538438\n",
            "At epoch 719, test loss: 120.633441538438\n",
            "At epoch 720, test loss: 120.633441538438\n",
            "At epoch 721, test loss: 120.633441538438\n",
            "At epoch 722, test loss: 120.633441538438\n",
            "At epoch 723, test loss: 120.633441538438\n",
            "At epoch 724, test loss: 120.6334416576473\n",
            "At epoch 725, test loss: 120.6334545321678\n",
            "At epoch 726, test loss: 120.6334545321678\n",
            "At epoch 727, test loss: 120.6336034134819\n",
            "At epoch 728, test loss: 120.6336034134819\n",
            "At epoch 729, test loss: 120.63836828282538\n",
            "At epoch 730, test loss: 120.6383828262532\n",
            "At epoch 731, test loss: 120.6383828262532\n",
            "At epoch 732, test loss: 120.6383828262532\n",
            "At epoch 733, test loss: 120.6383828262532\n",
            "At epoch 734, test loss: 120.6383966544353\n",
            "At epoch 735, test loss: 120.6383966544353\n",
            "At epoch 736, test loss: 120.63839725048157\n",
            "At epoch 737, test loss: 120.63840201884179\n",
            "At epoch 738, test loss: 120.6389666730249\n",
            "At epoch 739, test loss: 120.64025997239534\n",
            "At epoch 740, test loss: 120.64025997239534\n",
            "At epoch 741, test loss: 120.64025997239534\n",
            "At epoch 742, test loss: 120.64037154606942\n",
            "At epoch 743, test loss: 120.64037154606942\n",
            "At epoch 744, test loss: 120.64037154606942\n",
            "At epoch 745, test loss: 120.64037154606942\n",
            "At epoch 746, test loss: 120.64037154606942\n",
            "At epoch 747, test loss: 120.64037273816163\n",
            "At epoch 748, test loss: 120.64037273816163\n",
            "At epoch 749, test loss: 120.64037273816163\n",
            "At epoch 750, test loss: 120.64037285737092\n",
            "At epoch 751, test loss: 120.64043389066453\n",
            "At epoch 752, test loss: 120.64044271211284\n",
            "At epoch 753, test loss: 120.6404429505314\n",
            "At epoch 754, test loss: 120.6404436657869\n",
            "At epoch 755, test loss: 120.64049504367011\n",
            "At epoch 756, test loss: 120.64049504367011\n",
            "At epoch 757, test loss: 120.64049504367011\n",
            "At epoch 758, test loss: 120.64049504367011\n",
            "At epoch 759, test loss: 120.64049504367011\n",
            "At epoch 760, test loss: 120.64049504367011\n",
            "At epoch 761, test loss: 120.64049909677777\n",
            "At epoch 762, test loss: 120.64049909677777\n",
            "At epoch 763, test loss: 120.64049909677777\n",
            "At epoch 764, test loss: 120.64049909677777\n",
            "At epoch 765, test loss: 120.64049909677777\n",
            "At epoch 766, test loss: 120.64049909677777\n",
            "At epoch 767, test loss: 120.64091123754696\n",
            "At epoch 768, test loss: 120.64091207201164\n",
            "At epoch 769, test loss: 120.64110564915501\n",
            "At epoch 770, test loss: 120.64110564915501\n",
            "At epoch 771, test loss: 120.64110564915501\n",
            "At epoch 772, test loss: 120.64881551461332\n",
            "At epoch 773, test loss: 120.64883494553834\n",
            "At epoch 774, test loss: 120.64883494553834\n",
            "At epoch 775, test loss: 120.64938911518294\n",
            "At epoch 776, test loss: 120.64938911518294\n",
            "At epoch 777, test loss: 120.64941927467845\n",
            "At epoch 778, test loss: 122.79524966751598\n",
            "At epoch 779, test loss: 122.79524966751598\n",
            "At epoch 780, test loss: 122.79524966751598\n",
            "At epoch 781, test loss: 122.79530938958743\n",
            "At epoch 782, test loss: 122.7957745552365\n",
            "At epoch 783, test loss: 122.7957745552365\n",
            "At epoch 784, test loss: 122.7957745552365\n",
            "At epoch 785, test loss: 122.7957745552365\n",
            "At epoch 786, test loss: 122.7957745552365\n",
            "At epoch 787, test loss: 122.79578564163859\n",
            "At epoch 788, test loss: 122.79584095322099\n",
            "At epoch 789, test loss: 122.82653388821092\n",
            "At epoch 790, test loss: 122.82653388821092\n",
            "At epoch 791, test loss: 122.84389123760667\n",
            "At epoch 792, test loss: 122.84389135681596\n",
            "At epoch 793, test loss: 122.84389135681596\n",
            "At epoch 794, test loss: 122.84390089351365\n",
            "At epoch 795, test loss: 122.84390089351365\n",
            "At epoch 796, test loss: 122.84390089351365\n",
            "At epoch 797, test loss: 122.84390089351365\n",
            "At epoch 798, test loss: 122.84390256244234\n",
            "At epoch 799, test loss: 122.84390256244234\n",
            "At epoch 800, test loss: 122.84390256244234\n",
            "At epoch 801, test loss: 122.84390256244234\n",
            "At epoch 802, test loss: 122.88854423228196\n",
            "At epoch 803, test loss: 122.88854423228196\n",
            "At epoch 804, test loss: 122.88854435149125\n",
            "At epoch 805, test loss: 122.88854435149125\n",
            "At epoch 806, test loss: 122.88854435149125\n",
            "At epoch 807, test loss: 122.88854435149125\n",
            "At epoch 808, test loss: 122.93557555291346\n",
            "At epoch 809, test loss: 122.93557555291346\n",
            "At epoch 810, test loss: 122.93567949800791\n",
            "At epoch 811, test loss: 122.93567949800791\n",
            "At epoch 812, test loss: 122.96125268458428\n",
            "At epoch 813, test loss: 122.96125268458428\n",
            "At epoch 814, test loss: 122.96125268458428\n",
            "At epoch 815, test loss: 122.96150597225216\n",
            "At epoch 816, test loss: 122.96150597225216\n",
            "At epoch 817, test loss: 122.96150847564405\n",
            "At epoch 818, test loss: 122.96150847564405\n",
            "At epoch 819, test loss: 122.96150847564405\n",
            "At epoch 820, test loss: 122.96158381307743\n",
            "At epoch 821, test loss: 122.96158381307743\n",
            "At epoch 822, test loss: 122.96158381307743\n",
            "At epoch 823, test loss: 122.9615858396333\n",
            "At epoch 824, test loss: 123.56354620063718\n",
            "At epoch 825, test loss: 123.56354620063718\n",
            "At epoch 826, test loss: 123.56354691589269\n",
            "At epoch 827, test loss: 123.56354691589269\n",
            "At epoch 828, test loss: 123.56354751193896\n",
            "At epoch 829, test loss: 123.57474273664877\n",
            "At epoch 830, test loss: 123.57474810105225\n",
            "At epoch 831, test loss: 123.57474810105225\n",
            "At epoch 832, test loss: 123.88988727551362\n",
            "At epoch 833, test loss: 123.88988727551362\n",
            "At epoch 834, test loss: 123.88988727551362\n",
            "At epoch 835, test loss: 123.88989001732341\n",
            "At epoch 836, test loss: 123.89247542117744\n",
            "At epoch 837, test loss: 123.89247542117744\n",
            "At epoch 838, test loss: 123.89247542117744\n",
            "At epoch 839, test loss: 123.89247661326965\n",
            "At epoch 840, test loss: 123.89247661326965\n",
            "At epoch 841, test loss: 123.89247661326965\n",
            "At epoch 842, test loss: 123.89247661326965\n",
            "At epoch 843, test loss: 123.89247661326965\n",
            "At epoch 844, test loss: 123.89247661326965\n",
            "At epoch 845, test loss: 123.8924934216376\n",
            "At epoch 846, test loss: 123.89250152783654\n",
            "At epoch 847, test loss: 123.89250164704583\n",
            "At epoch 848, test loss: 123.89250164704583\n",
            "At epoch 849, test loss: 123.89250248151052\n",
            "At epoch 850, test loss: 123.89250438885733\n",
            "At epoch 851, test loss: 123.89250438885733\n",
            "At epoch 852, test loss: 123.89250450806662\n",
            "At epoch 853, test loss: 123.89250450806662\n",
            "At epoch 854, test loss: 123.89250450806662\n",
            "At epoch 855, test loss: 123.89250450806662\n",
            "At epoch 856, test loss: 123.89250450806662\n",
            "At epoch 857, test loss: 123.8925051041129\n",
            "At epoch 858, test loss: 124.47067433075932\n",
            "At epoch 859, test loss: 124.47068160249961\n",
            "At epoch 860, test loss: 124.47068160249961\n",
            "At epoch 861, test loss: 124.4707011526321\n",
            "At epoch 862, test loss: 124.47155754297142\n",
            "At epoch 863, test loss: 124.47170284854525\n",
            "At epoch 864, test loss: 124.47170284854525\n",
            "At epoch 865, test loss: 124.47170284854525\n",
            "At epoch 866, test loss: 124.62127304541224\n",
            "At epoch 867, test loss: 124.6212759064311\n",
            "At epoch 868, test loss: 124.62127602564038\n",
            "At epoch 869, test loss: 124.62127602564038\n",
            "At epoch 870, test loss: 124.62127686010507\n",
            "At epoch 871, test loss: 124.62127686010507\n",
            "At epoch 872, test loss: 124.62157019115119\n",
            "At epoch 873, test loss: 124.62157019115119\n",
            "At epoch 874, test loss: 124.6217282501836\n",
            "At epoch 875, test loss: 124.62172836939288\n",
            "At epoch 876, test loss: 124.62194089697158\n",
            "At epoch 877, test loss: 124.62194089697158\n",
            "At epoch 878, test loss: 124.71397517727172\n",
            "At epoch 879, test loss: 125.32764755295074\n",
            "At epoch 880, test loss: 125.32764755295074\n",
            "At epoch 881, test loss: 125.32764755295074\n",
            "At epoch 882, test loss: 125.32764767216003\n",
            "At epoch 883, test loss: 125.32764767216003\n",
            "At epoch 884, test loss: 125.32764767216003\n",
            "At epoch 885, test loss: 125.32764767216003\n",
            "At epoch 886, test loss: 125.32865948533589\n",
            "At epoch 887, test loss: 125.32867498242348\n",
            "At epoch 888, test loss: 125.32867510163277\n",
            "At epoch 889, test loss: 125.32867510163277\n",
            "At epoch 890, test loss: 125.32867510163277\n",
            "At epoch 891, test loss: 125.32867522084206\n",
            "At epoch 892, test loss: 125.3287934694651\n",
            "At epoch 893, test loss: 125.32914745839452\n",
            "At epoch 894, test loss: 125.32915079624917\n",
            "At epoch 895, test loss: 125.32920408138278\n",
            "At epoch 896, test loss: 125.3292536712176\n",
            "At epoch 897, test loss: 125.3292536712176\n",
            "At epoch 898, test loss: 125.3292536712176\n",
            "At epoch 899, test loss: 125.3292536712176\n",
            "At epoch 900, test loss: 125.32950040400436\n",
            "At epoch 901, test loss: 125.3299617567315\n",
            "At epoch 902, test loss: 125.3299618759408\n",
            "At epoch 903, test loss: 125.3299618759408\n",
            "At epoch 904, test loss: 125.32996294882383\n",
            "At epoch 905, test loss: 125.3317662697728\n",
            "At epoch 906, test loss: 125.3317662697728\n",
            "At epoch 907, test loss: 125.3317662697728\n",
            "At epoch 908, test loss: 125.33192981154485\n",
            "At epoch 909, test loss: 125.33192981154485\n",
            "At epoch 910, test loss: 125.33219215676132\n",
            "At epoch 911, test loss: 125.33219215676132\n",
            "At epoch 912, test loss: 125.33219227597061\n",
            "At epoch 913, test loss: 125.33219287201689\n",
            "At epoch 914, test loss: 125.33373865561487\n",
            "At epoch 915, test loss: 125.33376702702378\n",
            "At epoch 916, test loss: 125.33376702702378\n",
            "At epoch 917, test loss: 125.33378252411137\n",
            "At epoch 918, test loss: 125.33378252411137\n",
            "At epoch 919, test loss: 125.33378252411137\n",
            "At epoch 920, test loss: 125.33379527942441\n",
            "At epoch 921, test loss: 125.33379527942441\n",
            "At epoch 922, test loss: 125.33379527942441\n",
            "At epoch 923, test loss: 125.33379527942441\n",
            "At epoch 924, test loss: 125.33379527942441\n",
            "At epoch 925, test loss: 125.33383223362023\n",
            "At epoch 926, test loss: 125.33383223362023\n",
            "At epoch 927, test loss: 125.33383235282952\n",
            "At epoch 928, test loss: 125.33383235282952\n",
            "At epoch 929, test loss: 125.333835809893\n",
            "At epoch 930, test loss: 125.333835809893\n",
            "At epoch 931, test loss: 125.333835809893\n",
            "At epoch 932, test loss: 125.33383652514851\n",
            "At epoch 933, test loss: 125.33383652514851\n",
            "At epoch 934, test loss: 125.3483731890384\n",
            "At epoch 935, test loss: 125.34837855344188\n",
            "At epoch 936, test loss: 125.34837926869739\n",
            "At epoch 937, test loss: 125.34837926869739\n",
            "At epoch 938, test loss: 125.34839953407106\n",
            "At epoch 939, test loss: 125.34839953407106\n",
            "At epoch 940, test loss: 125.34839953407106\n",
            "At epoch 941, test loss: 126.70041085704622\n",
            "At epoch 942, test loss: 126.70041085704622\n",
            "At epoch 943, test loss: 126.70059287307129\n",
            "At epoch 944, test loss: 126.75127798450455\n",
            "At epoch 945, test loss: 126.75152197614705\n",
            "At epoch 946, test loss: 126.75152197614705\n",
            "At epoch 947, test loss: 126.75152769817657\n",
            "At epoch 948, test loss: 126.75152769817657\n",
            "At epoch 949, test loss: 126.75197129675223\n",
            "At epoch 950, test loss: 126.75197129675223\n",
            "At epoch 951, test loss: 126.75213066686854\n",
            "At epoch 952, test loss: 126.75213066686854\n",
            "At epoch 953, test loss: 126.75213066686854\n",
            "At epoch 954, test loss: 126.75213066686854\n",
            "At epoch 955, test loss: 126.75213066686854\n",
            "At epoch 956, test loss: 126.75213066686854\n",
            "At epoch 957, test loss: 126.75213066686854\n",
            "At epoch 958, test loss: 126.75213066686854\n",
            "At epoch 959, test loss: 126.75223127444485\n",
            "At epoch 960, test loss: 126.75267856684957\n",
            "At epoch 961, test loss: 126.75271182568838\n",
            "At epoch 962, test loss: 126.7527490182955\n",
            "At epoch 963, test loss: 126.75279836972265\n",
            "At epoch 964, test loss: 126.7527988465597\n",
            "At epoch 965, test loss: 126.7527988465597\n",
            "At epoch 966, test loss: 126.75279944260598\n",
            "At epoch 967, test loss: 126.75511014634736\n",
            "At epoch 968, test loss: 126.75511014634736\n",
            "At epoch 969, test loss: 126.75511014634736\n",
            "At epoch 970, test loss: 129.5151525753177\n",
            "At epoch 971, test loss: 129.5151525753177\n",
            "At epoch 972, test loss: 129.5151525753177\n",
            "At epoch 973, test loss: 129.5151525753177\n",
            "At epoch 974, test loss: 129.51515352899156\n",
            "At epoch 975, test loss: 129.51515352899156\n",
            "At epoch 976, test loss: 129.51515352899156\n",
            "At epoch 977, test loss: 129.51536021653743\n",
            "At epoch 978, test loss: 129.51536021653743\n",
            "At epoch 979, test loss: 129.51536021653743\n",
            "At epoch 980, test loss: 129.54061450150044\n",
            "At epoch 981, test loss: 129.54061450150044\n",
            "At epoch 982, test loss: 129.55722785640688\n",
            "At epoch 983, test loss: 129.55722785640688\n",
            "At epoch 984, test loss: 129.55722928691733\n",
            "At epoch 985, test loss: 129.55722928691733\n",
            "At epoch 986, test loss: 129.55727184372785\n",
            "At epoch 987, test loss: 129.55727315502912\n",
            "At epoch 988, test loss: 129.55727315502912\n",
            "At epoch 989, test loss: 129.55733049305303\n",
            "At epoch 990, test loss: 129.55733049305303\n",
            "At epoch 991, test loss: 129.55733049305303\n",
            "At epoch 992, test loss: 129.55733049305303\n",
            "At epoch 993, test loss: 129.55911049077434\n",
            "At epoch 994, test loss: 129.55911049077434\n",
            "At epoch 995, test loss: 129.55911049077434\n",
            "At epoch 996, test loss: 129.55911049077434\n",
            "At epoch 997, test loss: 129.56312502006216\n",
            "At epoch 998, test loss: 129.74296478012724\n",
            "At epoch 999, test loss: 129.74296871402606\n",
            "At epoch 1000, test loss: 129.7429736015951\n",
            "Test Loss: 0.129743\n",
            "\n",
            "Test Accuracy of 0: 98% (81/82)\n",
            "Test Accuracy of 1: 99% (116/117)\n",
            "Test Accuracy of 2: 99% (107/108)\n",
            "Test Accuracy of 3: 99% (111/112)\n",
            "Test Accuracy of 4: 100% (90/90)\n",
            "Test Accuracy of 5: 97% (95/97)\n",
            "Test Accuracy of 6: 95% (86/90)\n",
            "Test Accuracy of 7: 98% (109/111)\n",
            "Test Accuracy of 8: 94% (86/91)\n",
            "Test Accuracy of 9: 94% (96/102)\n",
            "\n",
            "Test Accuracy (Overall): 97% (977/1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dDMmsePi1OZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}