{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siya-9/Homomorphic-Encryption/blob/main/Logistic_Regression_using_FHE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FbCzQ-rlmyR",
        "outputId": "71bf9899-e5a6-4e52-c392-387cc83bcedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tenseal\n",
            "  Downloading tenseal-0.3.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/4.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/4.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/4.9 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenseal\n",
            "Successfully installed tenseal-0.3.14\n"
          ]
        }
      ],
      "source": [
        "!pip install tenseal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S3jd_IPl3aB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tenseal as ts\n",
        "import pandas as pd\n",
        "import random\n",
        "from time import time\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# those are optional and are not necessary for training\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rze9ggY05YkO",
        "outputId": "68839637-b898-43c4-e1d7-84348d4ec073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# 0) Prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "print(type(X), type(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exTzulzP5j-a",
        "outputId": "4e73faf5-653f-4344-fad3-f4b71d5c304d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-4484fb886e68>:8: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  data = data.drop(\"Exited\", 'columns')\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"bank.csv\")\n",
        "# drop rows with missing values\n",
        "data = data.dropna()\n",
        "data = data[:-5000]\n",
        "# drop some features\n",
        "data = data.drop(columns=[\"CustomerId\", \"Geography\"])\n",
        "y = data['Exited'].values\n",
        "data = data.drop(\"Exited\", 'columns')\n",
        "X = data.values\n",
        "\n",
        "print(type(X), type(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0BTFslJoJPQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=124)\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcZdGjxDof06"
      },
      "outputs": [],
      "source": [
        "class LRModel(torch.nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LRModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "model = LRModel(n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkEePKwSopf0"
      },
      "outputs": [],
      "source": [
        "num_epochs = 300\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.BCELoss()\n",
        "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm_pPMncouH7",
        "outputId": "8f57dbb8-d580-4dcb-d2de-669a02a53f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 10, loss = 0.6976\n",
            "accuracy: 0.5553\n",
            "epoch: 20, loss = 0.6866\n",
            "accuracy: 0.5687\n",
            "epoch: 30, loss = 0.6760\n",
            "accuracy: 0.5807\n",
            "epoch: 40, loss = 0.6660\n",
            "accuracy: 0.6027\n",
            "epoch: 50, loss = 0.6564\n",
            "accuracy: 0.6160\n",
            "epoch: 60, loss = 0.6473\n",
            "accuracy: 0.6293\n",
            "epoch: 70, loss = 0.6385\n",
            "accuracy: 0.6447\n",
            "epoch: 80, loss = 0.6302\n",
            "accuracy: 0.6533\n",
            "epoch: 90, loss = 0.6223\n",
            "accuracy: 0.6607\n",
            "epoch: 100, loss = 0.6148\n",
            "accuracy: 0.6720\n",
            "epoch: 110, loss = 0.6076\n",
            "accuracy: 0.6847\n",
            "epoch: 120, loss = 0.6007\n",
            "accuracy: 0.6927\n",
            "epoch: 130, loss = 0.5942\n",
            "accuracy: 0.7040\n",
            "epoch: 140, loss = 0.5879\n",
            "accuracy: 0.7100\n",
            "epoch: 150, loss = 0.5819\n",
            "accuracy: 0.7233\n",
            "epoch: 160, loss = 0.5763\n",
            "accuracy: 0.7313\n",
            "epoch: 170, loss = 0.5708\n",
            "accuracy: 0.7353\n",
            "epoch: 180, loss = 0.5656\n",
            "accuracy: 0.7433\n",
            "epoch: 190, loss = 0.5607\n",
            "accuracy: 0.7500\n",
            "epoch: 200, loss = 0.5560\n",
            "accuracy: 0.7533\n",
            "epoch: 210, loss = 0.5514\n",
            "accuracy: 0.7560\n",
            "epoch: 220, loss = 0.5471\n",
            "accuracy: 0.7593\n",
            "epoch: 230, loss = 0.5430\n",
            "accuracy: 0.7587\n",
            "epoch: 240, loss = 0.5390\n",
            "accuracy: 0.7613\n",
            "epoch: 250, loss = 0.5353\n",
            "accuracy: 0.7620\n",
            "epoch: 260, loss = 0.5316\n",
            "accuracy: 0.7653\n",
            "epoch: 270, loss = 0.5282\n",
            "accuracy: 0.7673\n",
            "epoch: 280, loss = 0.5248\n",
            "accuracy: 0.7673\n",
            "epoch: 290, loss = 0.5217\n",
            "accuracy: 0.7673\n",
            "epoch: 300, loss = 0.5186\n",
            "accuracy: 0.7687\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Backward pass and update\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    # zero grad before new step\n",
        "    optim.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "        y_predicted = model(X_test)\n",
        "        y_predicted_cls = y_predicted.round()\n",
        "        acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "        print(f'accuracy: {acc.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W9Cz_KQpY0W",
        "outputId": "f68b68be-3501-4301-9955-c0e2d8e2fe83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.7687\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    y_predicted = model(X_test)\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "    print(f'accuracy: {acc.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hONeDcdEpdjo"
      },
      "outputs": [],
      "source": [
        "class EncryptedLR:\n",
        "\n",
        "    def __init__(self, torch_lr):\n",
        "        # TenSEAL processes lists and not torch tensors,\n",
        "        # so we take out the parameters from the PyTorch model\n",
        "        self.weight = torch_lr.linear.weight.data.tolist()[0]\n",
        "        self.bias = torch_lr.linear.bias.data.tolist()\n",
        "\n",
        "    def forward(self, enc_x):\n",
        "        # We don't need to perform sigmoid as this model\n",
        "        # will only be used for evaluation, and the label\n",
        "        # can be deduced without applying sigmoid\n",
        "        enc_out = enc_x.dot(self.weight) + self.bias\n",
        "        return enc_out\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "    ################################################\n",
        "    ## You can use the functions below to perform ##\n",
        "    ## the evaluation with an encrypted model     ##\n",
        "    ################################################\n",
        "\n",
        "    def encrypt(self, context):\n",
        "        self.weight = ts.ckks_vector(context, self.weight)\n",
        "        self.bias = ts.ckks_vector(context, self.bias)\n",
        "\n",
        "    def decrypt(self, context):\n",
        "        self.weight = self.weight.decrypt()\n",
        "        self.bias = self.bias.decrypt()\n",
        "\n",
        "\n",
        "eelr = EncryptedLR(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0wLG612rHXL"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "poly_mod_degree = 4096\n",
        "coeff_mod_bit_sizes = [40, 20, 40]\n",
        "# create TenSEALContext\n",
        "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
        "# scale of ciphertext to use\n",
        "ctx_eval.global_scale = 2 ** 20\n",
        "# this key is needed for doing dot-product operations\n",
        "ctx_eval.generate_galois_keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-l0qPoHrv1K",
        "outputId": "6522bb8a-7bb0-4812-a5bf-d60e7d8795df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encryption of the test-set took 5 seconds\n"
          ]
        }
      ],
      "source": [
        "t_start = time()\n",
        "enc_x_test = [ts.ckks_vector(ctx_eval, x.tolist()) for x in X_test]\n",
        "t_end = time()\n",
        "print(f\"Encryption of the test-set took {int(t_end - t_start)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGSGPcL1rxgb",
        "outputId": "096a04aa-431a-4c68-e6c4-7521f9b070f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluated test_set of 5000 entries in 14 seconds\n",
            "Accuracy: 1150/1500 = 0.7666666666666667\n",
            "0.7666666666666667\n"
          ]
        }
      ],
      "source": [
        "def encrypted_evaluation(model, enc_x_test, y_test):\n",
        "    t_start = time()\n",
        "\n",
        "    correct = 0\n",
        "    for enc_x, y in zip(enc_x_test, y_test):\n",
        "        # encrypted evaluation\n",
        "        enc_out = model(enc_x)\n",
        "        # plain comparison\n",
        "        out = enc_out.decrypt()\n",
        "        out = torch.tensor(out)\n",
        "        out = torch.sigmoid(out)\n",
        "        if torch.abs(out - y) < 0.5:\n",
        "            correct += 1\n",
        "\n",
        "    t_end = time()\n",
        "    print(f\"Evaluated test_set of {len(X)} entries in {int(t_end - t_start)} seconds\")\n",
        "    print(f\"Accuracy: {correct}/{len(X_test)} = {correct / len(X_test)}\")\n",
        "    return correct / len(X_test)\n",
        "\n",
        "\n",
        "encrypted_accuracy = encrypted_evaluation(eelr, enc_x_test, y_test)\n",
        "print(encrypted_accuracy)\n",
        "# diff_accuracy = plain_accuracy - encrypted_accuracy\n",
        "# print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")\n",
        "# if diff_accuracy < 0:\n",
        "#     print(\"Oh! We got a better accuracy on the encrypted test-set! The noise was on our side...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAxT4WASsHxV"
      },
      "outputs": [],
      "source": [
        "class EncryptedLR:\n",
        "\n",
        "    def __init__(self, torch_lr):\n",
        "        self.weight = torch_lr.linear.weight.data.tolist()[0]\n",
        "        self.bias = torch_lr.linear.bias.data.tolist()\n",
        "        # we accumulate gradients and counts the number of iterations\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "        self._count = 0\n",
        "\n",
        "    def forward(self, enc_x):\n",
        "        enc_out = enc_x.dot(self.weight) + self.bias\n",
        "        enc_out = EncryptedLR.sigmoid(enc_out)\n",
        "        return enc_out\n",
        "\n",
        "    def backward(self, enc_x, enc_out, enc_y):\n",
        "        out_minus_y = (enc_out - enc_y)\n",
        "        self._delta_w += enc_x * out_minus_y\n",
        "        self._delta_b += out_minus_y\n",
        "        self._count += 1\n",
        "\n",
        "    def update_parameters(self):\n",
        "        if self._count == 0:\n",
        "            raise RuntimeError(\"You should at least run one forward iteration\")\n",
        "        # update weights\n",
        "        # We use a small regularization term to keep the output\n",
        "        # of the linear layer in the range of the sigmoid approximation\n",
        "        self.weight -= self._delta_w * (1 / self._count) + self.weight * 0.01\n",
        "        self.bias -= self._delta_b * (1 / self._count)\n",
        "        # reset gradient accumulators and iterations count\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "        self._count = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(enc_x):\n",
        "        # We use the polynomial approximation of degree 3\n",
        "        # sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3\n",
        "        # from https://eprint.iacr.org/2018/462.pdf\n",
        "        # which fits the function pretty well in the range [-5,5]\n",
        "        return enc_x.polyval([0.5, 0.197, 0, -0.004])\n",
        "\n",
        "    def plain_accuracy(self, x_test, y_test):\n",
        "        # evaluate accuracy of the model on\n",
        "        # the plain (x_test, y_test) dataset\n",
        "        w = torch.tensor(self.weight)\n",
        "        b = torch.tensor(self.bias)\n",
        "        out = torch.sigmoid(x_test.matmul(w) + b).reshape(-1, 1)\n",
        "        correct = torch.abs(y_test - out) < 0.5\n",
        "        return correct.float().mean()\n",
        "\n",
        "    def encrypt(self, context):\n",
        "        self.weight = ts.ckks_vector(context, self.weight)\n",
        "        self.bias = ts.ckks_vector(context, self.bias)\n",
        "\n",
        "    def decrypt(self):\n",
        "        self.weight = self.weight.decrypt()\n",
        "        self.bias = self.bias.decrypt()\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftVONlJIsWY8"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "poly_mod_degree = 8192\n",
        "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
        "# create TenSEALContext\n",
        "ctx_training = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
        "ctx_training.global_scale = 2 ** 21\n",
        "ctx_training.generate_galois_keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmihQeqosYiV",
        "outputId": "bcb1ef81-7079-4a1b-dccd-d6f90cff9975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encryption of the training_set took 103 seconds\n"
          ]
        }
      ],
      "source": [
        "t_start = time()\n",
        "enc_x_train = [ts.ckks_vector(ctx_training, x.tolist()) for x in X_train]\n",
        "enc_y_train = [ts.ckks_vector(ctx_training, y.tolist()) for y in y_train]\n",
        "t_end = time()\n",
        "print(f\"Encryption of the training_set took {int(t_end - t_start)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1k2lElvsbax"
      },
      "outputs": [],
      "source": [
        "# normal_dist = lambda x, mean, var: np.exp(- np.square(x - mean) / (2 * var)) / np.sqrt(2 * np.pi * var)\n",
        "\n",
        "# def plot_normal_dist(mean, var, rmin=-10, rmax=10):\n",
        "#     x = np.arange(rmin, rmax, 0.01)\n",
        "#     y = normal_dist(x, mean, var)\n",
        "#     fig = plt.plot(x, y)\n",
        "\n",
        "# # plain distribution\n",
        "# lr = LRModel(n_features)\n",
        "# data = lr.linear(X_test)\n",
        "# mean, var = map(float, [data.mean(), data.std() ** 2])\n",
        "# plot_normal_dist(mean, var)\n",
        "# print(\"Distribution on plain data:\")\n",
        "# plt.show()\n",
        "\n",
        "# # encrypted distribution\n",
        "# def encrypted_out_distribution(eelr, enc_x_test):\n",
        "#     w = eelr.weight\n",
        "#     b = eelr.bias\n",
        "#     data = []\n",
        "#     for enc_x in enc_x_test:\n",
        "#         enc_out = enc_x.dot(w) + b\n",
        "#         data.append(enc_out.decrypt())\n",
        "#     data = torch.tensor(data)\n",
        "#     mean, var = map(float, [data.mean(), data.std() ** 2])\n",
        "#     plot_normal_dist(mean, var)\n",
        "#     print(\"Distribution on encrypted data:\")\n",
        "#     plt.show()\n",
        "\n",
        "# eelr = EncryptedLR(lr)\n",
        "# eelr.encrypt(ctx_training)\n",
        "# encrypted_out_distribution(eelr, enc_x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGpgYyyrsiEQ",
        "outputId": "0fcdd2c8-3dca-4f29-b587-d6b36564604c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy at epoch #0 is 0.3293333351612091\n",
            "Accuracy at epoch #1 is 0.6959999799728394\n",
            "Accuracy at epoch #2 is 0.7613333463668823\n",
            "Accuracy at epoch #3 is 0.7860000133514404\n",
            "Accuracy at epoch #4 is 0.7893333435058594\n",
            "Accuracy at epoch #5 is 0.7860000133514404\n",
            "\n",
            "Average time per epoch: 459 seconds\n",
            "Final accuracy is 0.7860000133514404\n"
          ]
        }
      ],
      "source": [
        "\n",
        "eelr = EncryptedLR(LRModel(n_features))\n",
        "accuracy = eelr.plain_accuracy(X_test, y_test)\n",
        "print(f\"Accuracy at epoch #0 is {accuracy}\")\n",
        "\n",
        "times = []\n",
        "for epoch in range(5):\n",
        "    eelr.encrypt(ctx_training)\n",
        "\n",
        "    # if you want to keep an eye on the distribution to make sure\n",
        "    # the function approximation is still working fine\n",
        "    # WARNING: this operation is time consuming\n",
        "    # encrypted_out_distribution(eelr, enc_x_train)\n",
        "\n",
        "    t_start = time()\n",
        "    for enc_x, enc_y in zip(enc_x_train, enc_y_train):\n",
        "        enc_out = eelr.forward(enc_x)\n",
        "        eelr.backward(enc_x, enc_out, enc_y)\n",
        "    eelr.update_parameters()\n",
        "    t_end = time()\n",
        "    times.append(t_end - t_start)\n",
        "\n",
        "    eelr.decrypt()\n",
        "    accuracy = eelr.plain_accuracy(X_test, y_test)\n",
        "    print(f\"Accuracy at epoch #{epoch + 1} is {accuracy}\")\n",
        "\n",
        "\n",
        "print(f\"\\nAverage time per epoch: {int(sum(times) / len(times))} seconds\")\n",
        "print(f\"Final accuracy is {accuracy}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KPDrCAgrxKKx"
      },
      "outputs": [],
      "source": [
        "plain_accuracy = 0.7687\n",
        "accuracy = 0.7860"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEXe8T_nUWet",
        "outputId": "340438bc-9b3b-466c-8930-124fe5353d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference between plain and encrypted accuracies: -0.017299999999999982\n",
            "We got a better accuracy when training on encrypted data\n"
          ]
        }
      ],
      "source": [
        "diff_accuracy = plain_accuracy - accuracy\n",
        "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")\n",
        "if diff_accuracy < 0:\n",
        "    print(\"We got a better accuracy when training on encrypted data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jhv7jxtQUejS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMCpAaqtRN86GYI+gQSZKv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}